{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on Tuesday, April 13th, 2020, 6:29pm PST. You may work with one other person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are an analyst working at McDonalds as a store operations analyst, and charged with identifying areas for improvement for each franchise. Several metropolitan locations have been suffering recently from lower reviews.\n",
    "\n",
    "Using the **mcdonalds-yelp-negative-reviews.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "- which words to collocate together? (optional)\n",
    "\n",
    "Finally, generate a TF-IDF report that either **visualizes** or explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Attribution (Feature Engineering and Regex Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [dataset](https://dso-560-nlp-text-analytics.s3.amazonaws.com/truncated_catalog.csv) from the class S3 bucket (`dso560-nlp-text-analytics`).\n",
    "\n",
    "In preparation for the group project, our client company has provided a dataset of women's clothing products they are considering cataloging. \n",
    "\n",
    "1. Filter for only **women's clothing items**.\n",
    "\n",
    "2. For each clothing item:\n",
    "\n",
    "* Identify its **category**:\n",
    "```\n",
    "Bottom\n",
    "One Piece\n",
    "Shoe\n",
    "Handbag\n",
    "Scarf\n",
    "```\n",
    "* Identify its **color**:\n",
    "```\n",
    "Beige\n",
    "Black\n",
    "Blue\n",
    "Brown\n",
    "Burgundy\n",
    "Gold\n",
    "Gray\n",
    "Green\n",
    "Multi \n",
    "Navy\n",
    "Neutral\n",
    "Orange\n",
    "Pinks\n",
    "Purple\n",
    "Red\n",
    "Silver\n",
    "Teal\n",
    "White\n",
    "Yellow\n",
    "```\n",
    "\n",
    "Your output will be the same dataset, except with **3 additional fields**:\n",
    "* `is_womens_clothing`\n",
    "* `product_category`\n",
    "* `colors`\n",
    "\n",
    "`colors` should be a list of colors, since it is possible for a piece of clothing to have multiple colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:11.275708Z",
     "start_time": "2021-04-14T01:10:05.359732Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "df = pd.read_csv('mcdonalds-yelp-negative-reviews.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary regex cleaning\n",
    "\n",
    "Before any analysis or further processing of data, I performed the following preliminary regex cleaning:\n",
    "\n",
    "- made all words lowercase\n",
    "- changed all negative meaning words (doesn't, isn't, hadn't, etc.) to \"not\" (We may expect a lot of words of this kind since this is a negative review dataset, changing them into \"not\" preserves their negative meanings while reducing their variations)\n",
    "- standardized different spellings of McDonalds\n",
    "- removed punctuations\n",
    "\n",
    "All of these cleaning were to make it easier later in the analysis to recognize useful words/information without worrying about the variations of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:11.479585Z",
     "start_time": "2021-04-14T01:10:11.279596Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean up text\n",
    "# lowercase everything\n",
    "df['review'] = df['review'].str.lower()\n",
    "\n",
    "# change all words such as \"doesn't\", \"hadn't\" into \"not\" to preserve their negative meanings while reducing variations\n",
    "df['review'] = df['review'].str.replace(r\"\\b\\w+n't\\b\",'not')\n",
    "\n",
    "# standardize the spelling of McDonalds\n",
    "df['review'] = df['review'].str.replace(r\"\\b(?:mc ?donald(?:s|'s)?|mcds?)\\b\",'mcdonalds')\n",
    "\n",
    "# remove punctuations\n",
    "df['review'] = df['review'].str.replace(r'[^\\w\\s]', ' ')\n",
    "\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "\n",
    "I removed the stopwords for the following reasons:\n",
    "\n",
    "1. Our end goal is to identify the most common issues of McDonalds from the negative reviews, but the stopwords are very common words that do not add to our knowledge about McDonald's issues, meaning they will appear as frequently occurring words but with **no valuable information on the meaning of the texts**;\n",
    "\n",
    "2. By removing stopwords, we can **reduce dimensionality** in our TF-IDF analysis and also reduce computational cost.\n",
    "\n",
    "What stopwords to remove:\n",
    "\n",
    "- Other than the common **\"english\"** stopwords from ```nltk```, I added **\"mcdonalds\", \"absolute\", \"absolutely\", and \"really\"** as custom stopwords.\n",
    "\n",
    "- For \"mcdonalds\", we already know that the dataset is about McDonalds, so making it a stopword will not reduce useful information.\n",
    "\n",
    "- For words like \"absolute\", \"absolutely\", and \"really\", since this is a negative review dataset, people's reviews may express lots of emotions using these words, so these words only describe the extent or degree of some emotions but do not provide additional information about the actual content, i.e. what is being negatively reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:11.509955Z",
     "start_time": "2021-04-14T01:10:11.485568Z"
    }
   },
   "outputs": [],
   "source": [
    "# modify stopwords list\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# knowing this is a review dataset about mcdonalds, we can make \"mcdonalds\" a stopword for not providing additional info\n",
    "stopwords_list = stopwords_list + ['mcdonalds']\n",
    "# additional stopwords: these words only describe the extent/degree of something, do not provide info about the content\n",
    "stopwords_list = stopwords_list + ['absolute','absolutely','really']\n",
    "\n",
    "len(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:11.524201Z",
     "start_time": "2021-04-14T01:10:11.515259Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to remove stopwords\n",
    "def remove_sw(text):\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # split sentence into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    new_words = []\n",
    "    # remove stopwords\n",
    "    for w in words:\n",
    "        if w in stopwords_list:\n",
    "            continue\n",
    "        new_words.append(w)\n",
    "    \n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further regex cleaning and substitution\n",
    "\n",
    "After removing stopwords, I performed some more regex cleaning to reduce word variations:\n",
    "\n",
    "- different spellings/types of \"burgers\" were changed into \"burger\"\n",
    "- words like \"aaahhhh\" were removed\n",
    "- words like \"noooo\" were changed to \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:14.045456Z",
     "start_time": "2021-04-14T01:10:11.529270Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleaned review without stopwords\n",
    "df['cleaned_review'] = df['review'].apply(lambda x: remove_sw(x))\n",
    "\n",
    "# further clean up text\n",
    "df['cleaned_review'] = df['cleaned_review'].str.replace(r'\\b(?:ham|beef|bacon)? ?burgers?\\b','burger')\n",
    "df['cleaned_review'] = df['cleaned_review'].str.replace(r'\\b(?:a+|u+)h+\\b','')\n",
    "df['cleaned_review'] = df['cleaned_review'].str.replace(r'\\bn+?o+?\\b','no')\n",
    "\n",
    "df['cleaned_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "I chose to perform lemmatization instead of stemming on the reviews mainly because at the end of the analysis, we want to identify and understand the specific issues of McDonald's, and lemmatization will return the **actual and meaningful words considering the context** rather than the stems only, so it would make it much easier than stemming to understand the tokens/words returned by our final analysis and to summarize McDonald's issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:32.622280Z",
     "start_time": "2021-04-14T01:10:14.050443Z"
    }
   },
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "# reference: https://gist.github.com/gaurav5430/9fce93759eb2f6b1697883c3782f30de#file-nltk-lemmatize-sentences-py\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return ' '.join(lemmatized_sentence)\n",
    "\n",
    "df['lemma'] = df['cleaned_review'].apply(lambda x: lemmatize_sentence(x))\n",
    "df['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "For the TF-IDF vectorizer, I used **bi- and trigrams (n = 2 and n = 3)** because I believe these two-word and three-word phrases are indicative enough to allow us to understand the meaning of the \"local\" text without getting too detailed.\n",
    "\n",
    "I also set the token pattern to only consider words with 3 or more characters and set the maximum occurrences of a token to be 50% of all documents. Too frequently occurring tokens are not unique enough for us to understand the meaning of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:34.188464Z",
     "start_time": "2021-04-14T01:10:32.628277Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,3),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df=0.5)\n",
    "\n",
    "X = vectorizer.fit_transform(df['lemma'])\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:45.294427Z",
     "start_time": "2021-04-14T01:10:34.194403Z"
    }
   },
   "outputs": [],
   "source": [
    "# add up the tf-idf score of each document for each token and sort in descending order of the token tf-idf scores\n",
    "tf_idf_terms = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf_terms, columns=['score'])\n",
    "score.sort_values(by='score', ascending=False, inplace=True)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:47.062447Z",
     "start_time": "2021-04-14T01:10:45.302405Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the top tokens\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# data\n",
    "xs = score.head(25).index\n",
    "ys = score.head(25)['score']\n",
    "\n",
    "# main plot\n",
    "fig, ax = plt.subplots(figsize=(17,7))\n",
    "ax.bar(xs,ys,color='#2d4673',width=0.6)\n",
    "\n",
    "# x-axis gridline\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# x,y range\n",
    "plt.margins(x=0.02)\n",
    "\n",
    "# x,y ticks\n",
    "plt.xticks(xs,rotation=45,ha='right',fontsize=17,color='black')\n",
    "plt.yticks(fontsize=17,color='black')\n",
    "plt.tick_params(axis=\"both\",which=\"both\",bottom=False,top=False,left=False,right=False)\n",
    "\n",
    "# x,y labels\n",
    "plt.ylabel('TF-IDF',fontsize=20,color='black',labelpad=10)\n",
    "plt.xlabel('Feature terms',fontsize=20,color='black',labelpad=10)\n",
    "\n",
    "# title\n",
    "plt.title('Top 25 Feature Terms by TF-IDF Score',fontsize=22)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization above, we can see that customers frequently cited feature terms such as **drive thru, get/take/place order right/wrong, wait minute, and different types of food (ice cream, big mac, chicken nuggets, french fries)** as reasons for giving negative reviews. In the following steps, I would manually look through the documents containing some of these feature terms to further summarize the specific issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:47.131815Z",
     "start_time": "2021-04-14T01:10:47.068332Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['lemma'].str.contains(r'\\bdrive thru\\b'),'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:47.192299Z",
     "start_time": "2021-04-14T01:10:47.136764Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['lemma'].str.contains(r'\\b(?:get|take|place) order (?:right|wrong)?|order (?:right|wrong)\\b'),'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manually looking at the documents containing \"drive thru\" (325 reviews), the common issues I saw include **unclear instructions at the speaker, long waiting time, unfriendly communication (e.g. no smiles), and unprofessional handling of orders (e.g. not wearing gloves and serve with bare hands). \"Wrong order\"** is another very common issue cited in these reviews, so I also checked the documents containing feature terms related to getting/taking/placing orders right/wrong (241 reviews). Issues related to ordering include **ordering wrong items multiple times, disorganized service, etc.**\n",
    "\n",
    "For these specific issues, I really believe that McDonald's operations team should **schedule regular employee trainings and evaluations with a system of rewards and penalties.** Training and evaluation areas should include customer communication, order handling, and teamwork/streamline service. Getting the order right should be the most basic requirement for the employees, and it ties into handling orders fast and well. Good communication with customers is also a foundation to take orders correctly or make customers satisfied and happy even when some small mistakes occur occasionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on TF-IDF Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF **balances between term frequency and document frequency**, meaning that to have a high TF-IDF score, a token/term needs to appear not in too many documents within the corpus, but when it appears, it appears a lot of times in that document. Given this consideration, I think TF-IDF makes better sense than a pure word-count analysis in this case of analyzing reviews, as it can help to identify the relatively more **important, relevant, and descriptive keywords and issues**.\n",
    "\n",
    "However, TF-IDF has its own limitations. While it considers both term frequency and document frequency of a token/term, it is still analyzing this single token/term **without capturing its position in text, semantics, and sentiment**. For example, from the TF-IDF analysis above, I knew that \"drive thru\" is an important and relevant term cited in the reviews, but I still need to manually look through the documents containing \"drive thru\" to understand the common sentiment and what common issues occur at the drive-thrus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations (optional)\n",
    "\n",
    "I took a look at the top 30 collocating bigrams, and I think that in future analyses, we may combine these words into one token, so when we perform the bi- and trigram TF-IDF analysis again, these tokens would be read together with one or two other words to add to our knowledge about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:48.145921Z",
     "start_time": "2021-04-14T01:10:47.194340Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "documents_words = []\n",
    "for i in range(len(df)):\n",
    "    documents_words.append(word_tokenize(df.loc[i,'lemma']))\n",
    "    \n",
    "len(documents_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:49.050698Z",
     "start_time": "2021-04-14T01:10:48.150260Z"
    }
   },
   "outputs": [],
   "source": [
    "collocation_finder = BigramCollocationFinder.from_documents(documents_words)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Attribution (Feature Engineering and Regex Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:49.985416Z",
     "start_time": "2021-04-14T01:10:49.054686Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "catalog = pd.read_csv('truncated_catalog.csv')\n",
    "catalog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:53.507578Z",
     "start_time": "2021-04-14T01:10:49.990354Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a copy of the original dataset\n",
    "catalog_copy = catalog.copy()\n",
    "\n",
    "# fill missing values\n",
    "catalog.fillna('None', inplace=True)\n",
    "\n",
    "# clean up columns that are helpful for tagging categories and colors\n",
    "# remove punctuations, \\n, etc. to better recognize keywords\n",
    "for col in ['name', 'description', 'brand_category', 'brand_canonical_url', 'details']:\n",
    "    catalog[col] = catalog[col].str.replace('\\n', ' ')\n",
    "    catalog[col] = catalog[col].str.replace('\\r', ' ')\n",
    "    catalog[col] = catalog[col].str.replace(r'https:\\/\\/(?:www\\.)?|\\.com', '')\n",
    "    catalog[col] = catalog[col].str.replace(r'[^\\w\\s]|_', ' ')\n",
    "\n",
    "# combine useful columns into a single column, so we only need to look for keywords in this single column \n",
    "catalog['concat_description'] = catalog['name'] + ' ' + catalog['description'] + ' ' \\\n",
    "                                + catalog['brand_category'] + ' ' + catalog['brand_canonical_url'] + ' ' \\\n",
    "                                + catalog['details']\n",
    "\n",
    "catalog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_womens_clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:57.234725Z",
     "start_time": "2021-04-14T01:10:53.513013Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# find out whether each item description contains words referring to women's clothing\n",
    "catalog['is_womens_clothing'] = catalog['concat_description'].str.\\\n",
    "                                contains(\n",
    "                                r\"\\b(?:woman|women|girl|female|lady)(?:s|'s)?|ladies|dress(?:es)?|skirts?|purses?\\b\",\n",
    "                                flags=re.IGNORECASE)\n",
    "\n",
    "catalog[['is_womens_clothing']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:57.250637Z",
     "start_time": "2021-04-14T01:10:57.238666Z"
    }
   },
   "outputs": [],
   "source": [
    "# 13172 items are labeled as women's clothing\n",
    "catalog['is_womens_clothing'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### product_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method I used to label product categories may seem complicated, but i think it is the method that makes the most sense to me.\n",
    "\n",
    "- I defined a function that takes into the dataframe, the text column to search through, and the category name (i.e. tag) and would return five boolean columns, one for each category. If a document contains keywords of a category, this category column will have \"True\" otherwise \"False\" for this document.\n",
    "\n",
    "- For all items that contain keywords of exactly one category, I directly labeled them with this category name.\n",
    "\n",
    "- For items that contain keywords of more than one category, I reran the function using a more specific text column to search for keywords and narrow down their potential categories.\n",
    "\n",
    "- This process was repeated until all items were assigned with a reasonable category label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:10:57.275567Z",
     "start_time": "2021-04-14T01:10:57.256621Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to recognize item category\n",
    "def category_tag(df, col, tag):\n",
    "    if tag=='Bottom':\n",
    "        df[tag] = df[col].str.contains(\n",
    "            r\"\\b(?:pant|jean|trouser|short|tight)s|(?:skirt|legging|palazzo|skort|wide leg)s?\\b\",\n",
    "            flags=re.IGNORECASE)\n",
    "    elif tag=='One Piece':\n",
    "        df[tag] = df[col].str.contains(\n",
    "            r\"\\b(?:one ?piece|onesie|gown|robe|romper|(?:body|jump|boiler) ?suit|shortall|c?over ?all)s?|dress(?:es)?\\b\",\n",
    "            flags=re.IGNORECASE)\n",
    "    elif tag=='Shoe':\n",
    "        df[tag] = df[col].str.contains(\n",
    "        r\"\\b(?:shoe|sneaker|sandal|heel|boot|slipper|flip ?flop|trainer|platform|oxford|mule|brogue|loafer|moccasin|derby)s?|flats|derbies\\b\",\n",
    "        flags=re.IGNORECASE)\n",
    "    elif tag=='Handbag':\n",
    "        df[tag] = df[col].str.contains(\n",
    "            r\"\\b(?:hand|saddle|bucket|frame) ?bags?|(?:purse|clutch|hobo|tote)(?:es|s)?\\b\",\n",
    "            flags=re.IGNORECASE)\n",
    "    else:\n",
    "        df[tag] = df[col].str.contains(\n",
    "            r\"\\b(?:scar|neckerchie)(?:fs?|ves)|(?:muffler|shawl|pashmina|bandana|stole)s?\\b\",\n",
    "            flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:11:22.442585Z",
     "start_time": "2021-04-14T01:10:57.280554Z"
    }
   },
   "outputs": [],
   "source": [
    "# for each item, find out whether it contains the keywords of each category\n",
    "for tag in ['Bottom','One Piece','Shoe','Handbag','Scarf']:\n",
    "    category_tag(catalog, 'concat_description', tag)\n",
    "    \n",
    "catalog[['Bottom','One Piece','Shoe','Handbag','Scarf']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:11:22.475617Z",
     "start_time": "2021-04-14T01:11:22.446588Z"
    }
   },
   "outputs": [],
   "source": [
    "# some items may contain keywords of more than one category, check the sum of the category columns to be sure\n",
    "catalog['cat_sum'] = catalog[['Bottom','One Piece','Shoe','Handbag','Scarf']].sum(axis=1)\n",
    "catalog[['cat_sum']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:11:59.003888Z",
     "start_time": "2021-04-14T01:11:22.479606Z"
    }
   },
   "outputs": [],
   "source": [
    "# first, deal with the items that have keywords in exactly one category or no categories\n",
    "catalog['product_category'] = np.nan\n",
    "\n",
    "for i in catalog.loc[catalog['cat_sum']<=1].index:\n",
    "    for tag in ['Bottom','One Piece','Shoe','Handbag','Scarf']:\n",
    "        if catalog.loc[i,tag]:\n",
    "            catalog.loc[i,'product_category'] = tag\n",
    "            \n",
    "catalog[['product_category']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:11:59.029218Z",
     "start_time": "2021-04-14T01:11:59.007884Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the items that contain keywords of more than one category\n",
    "multi_cat = catalog.loc[catalog['cat_sum']>1,['brand','name','description',\n",
    "                                              'brand_category','brand_canonical_url',\n",
    "                                              'details','concat_description']].copy()\n",
    "multi_cat.shape\n",
    "# 2289 items have keywords of more than one category, need to treat them specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:04.084025Z",
     "start_time": "2021-04-14T01:11:59.041421Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"name\" is the most accurate column to determine the actual category, rerun tagging using the \"name\" column\n",
    "for tag in ['Bottom','One Piece','Shoe','Handbag','Scarf']:\n",
    "    category_tag(multi_cat, 'name', tag)\n",
    "multi_cat['cat_sum'] = multi_cat[['Bottom','One Piece','Shoe','Handbag','Scarf']].sum(axis=1)\n",
    "\n",
    "# recheck items that have keywords of exactly one category\n",
    "for i in multi_cat.loc[multi_cat['cat_sum']==1].index:\n",
    "    for tag in ['Bottom','One Piece','Shoe','Handbag','Scarf']:\n",
    "        if multi_cat.loc[i,tag]:\n",
    "            multi_cat.loc[i,'product_category'] = tag\n",
    "            catalog.loc[i,'product_category'] = tag\n",
    "            \n",
    "multi_cat[['product_category']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:04.108942Z",
     "start_time": "2021-04-14T01:12:04.092609Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 635 more items to tag\n",
    "multi_cat = multi_cat.loc[multi_cat['product_category'].isnull(),['brand','name','description',\n",
    "                                                                  'brand_category','brand_canonical_url',\n",
    "                                                                  'details','concat_description']].copy()\n",
    "multi_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:05.039689Z",
     "start_time": "2021-04-14T01:12:04.112790Z"
    }
   },
   "outputs": [],
   "source": [
    "# use \"brand_category\" to tag\n",
    "for tag in ['Bottom','One Piece','Shoe','Handbag','Scarf']:\n",
    "    category_tag(multi_cat, 'brand_category', tag)\n",
    "multi_cat['cat_sum'] = multi_cat[['Bottom','One Piece','Shoe','Handbag','Scarf']].sum(axis=1)\n",
    "\n",
    "for i in multi_cat.loc[multi_cat['cat_sum']==1].index:\n",
    "    for tag in ['Bottom','One Piece','Shoe','Handbag','Scarf']:\n",
    "        if multi_cat.loc[i,tag]:\n",
    "            multi_cat.loc[i,'product_category'] = tag\n",
    "            catalog.loc[i,'product_category'] = tag\n",
    "            \n",
    "multi_cat[['product_category']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:05.060726Z",
     "start_time": "2021-04-14T01:12:05.043567Z"
    }
   },
   "outputs": [],
   "source": [
    "# 352 more items to tag\n",
    "multi_cat = multi_cat.loc[multi_cat['product_category'].isnull(),['brand','name','description',\n",
    "                                                                  'brand_category','brand_canonical_url',\n",
    "                                                                  'details','concat_description']].copy()\n",
    "multi_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:05.257543Z",
     "start_time": "2021-04-14T01:12:05.064238Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the last word in the name is usually the actual item\n",
    "multi_cat['name_last'] = multi_cat['name'].str.split().apply(lambda x: x[-1])\n",
    "\n",
    "for tag in ['Bottom','One Piece','Shoe','Handbag','Scarf']:\n",
    "    category_tag(multi_cat, 'name_last', tag)\n",
    "multi_cat['cat_sum'] = multi_cat[['Bottom','One Piece','Shoe','Handbag','Scarf']].sum(axis=1)\n",
    "\n",
    "for i in multi_cat.loc[multi_cat['cat_sum']==1].index:\n",
    "    for tag in ['Bottom','One Piece','Shoe','Handbag','Scarf']:\n",
    "        if multi_cat.loc[i,tag]:\n",
    "            multi_cat.loc[i,'product_category'] = tag\n",
    "            catalog.loc[i,'product_category'] = tag\n",
    "            \n",
    "multi_cat[['product_category']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:05.275859Z",
     "start_time": "2021-04-14T01:12:05.260615Z"
    }
   },
   "outputs": [],
   "source": [
    "# 305 more items to tag\n",
    "multi_cat = multi_cat.loc[multi_cat['product_category'].isnull(),['brand','name','description',\n",
    "                                                                  'brand_category','brand_canonical_url',\n",
    "                                                                  'details','concat_description']].copy()\n",
    "multi_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:05.310921Z",
     "start_time": "2021-04-14T01:12:05.281000Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove items in undefined categories - 16 items left\n",
    "multi_cat['other'] = multi_cat['concat_description'].str.contains(\n",
    "    r\"\\b(?:shirt|tee|tshirt|top|sweater|jacket|blouse|coat|sock|polo|belt|blazer|pullover|turtleneck|earring|glove|tank|sweatshirt|necklace|set)s?\\b\",\n",
    "    flags=re.IGNORECASE)\n",
    "\n",
    "multi_cat = multi_cat.loc[multi_cat['other']==False]\n",
    "multi_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:05.372756Z",
     "start_time": "2021-04-14T01:12:05.313913Z"
    }
   },
   "outputs": [],
   "source": [
    "# manually assign categories after manually looking through the descriptions\n",
    "for i in multi_cat.index:\n",
    "    \n",
    "    if multi_cat.loc[i,'brand_category'] == 'dressesandjumpsuits' or \\\n",
    "    'SWIMWEAR' in multi_cat.loc[i,'brand_category'].split() or \\\n",
    "    'FootiesRompers' in multi_cat.loc[i,'brand_category'].split():\n",
    "        multi_cat.loc[i,'product_category'] = 'One Piece'\n",
    "        catalog.loc[i,'product_category'] = 'One Piece'\n",
    "    \n",
    "    elif 'Pant' in multi_cat.loc[i,'name'].split() or 'pant' in multi_cat.loc[i,'name'].split():\n",
    "        multi_cat.loc[i,'product_category'] = 'Bottom'\n",
    "        catalog.loc[i,'product_category'] = 'Bottom'\n",
    "    \n",
    "    elif 'Flat' in multi_cat.loc[i,'name'].split():\n",
    "        multi_cat.loc[i,'product_category'] = 'Shoe'\n",
    "        catalog.loc[i,'product_category'] = 'Shoe'\n",
    "    \n",
    "    elif 'clutch' in multi_cat.loc[i,'description'].split() or \\\n",
    "    'saddlebag' in multi_cat.loc[i,'description'].split() or \\\n",
    "    'purse' in multi_cat.loc[i,'description'].split():\n",
    "        multi_cat.loc[i,'product_category'] = 'Handbag'\n",
    "        catalog.loc[i,'product_category'] = 'Handbag'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:24.219619Z",
     "start_time": "2021-04-14T01:12:05.375858Z"
    }
   },
   "outputs": [],
   "source": [
    "# find all colors in item descriptions\n",
    "catalog['color_list'] = catalog['concat_description'].str.findall(\n",
    "    r\"\\bbeige|light brown|black|blue ?green|blue|brown|umber|burgundy|gold(?:en)?|gray|grey|green|navy|neutral|orange|aurantia|pink|purple|violet|red|scarlet|silver|teal|white|yellow|(?:multi(?:ple)?|several|different|many|more than one) ?colou?rs?\\b\",\n",
    "    flags=re.IGNORECASE).apply(lambda x: [w.lower() for w in x]).apply(lambda x: set(x))\n",
    "# standardize all words to lowercase and make the list a set to avoid repeated colors\n",
    "\n",
    "# number of unique colors in the set\n",
    "catalog['n_colors'] = catalog['color_list'].apply(len)\n",
    "catalog[['color_list','n_colors']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:24.295455Z",
     "start_time": "2021-04-14T01:12:24.221611Z"
    }
   },
   "outputs": [],
   "source": [
    "# tag color labels\n",
    "catalog['colors'] = np.nan\n",
    "\n",
    "# label items with more than one color as \"Multi\"\n",
    "catalog.loc[catalog['n_colors']>1,'colors'] = 'Multi'\n",
    "\n",
    "# label items with one color\n",
    "catalog.loc[catalog['n_colors']==1,'colors'] = catalog.loc[catalog['n_colors']==1,'color_list'].\\\n",
    "                                                                        apply(lambda x:list(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:24.414967Z",
     "start_time": "2021-04-14T01:12:24.299281Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to standardize color labels\n",
    "def color_tag(text):\n",
    "    \n",
    "    color = text\n",
    "    \n",
    "    if isinstance(text, str):\n",
    "\n",
    "        if text == 'beige' or text == 'light brown':\n",
    "            color = 'Beige'\n",
    "        elif text == 'blue green' or text == 'bluegreen' or text == 'teal':\n",
    "            color = 'Teal'\n",
    "        elif text == 'brown' or text == 'umber':\n",
    "            color = 'Brown'\n",
    "        elif text == 'gold' or text == 'golden':\n",
    "            color = 'Gold'\n",
    "        elif text == 'gray' or text == 'grey':\n",
    "            color = 'Gray'\n",
    "        elif text == 'orange' or text == 'aurantia':\n",
    "            color = 'Orange'\n",
    "        elif text == 'purple' or text == 'violet':\n",
    "            color = 'Purple'\n",
    "        elif text == 'red' or text == 'scarlet':\n",
    "            color = 'Red'\n",
    "        elif re.findall(r\"(?:multi(?:ple)?|several|different|many|more than one) ?colou?rs?\\b\", text) != []:\n",
    "            color = 'Multi'\n",
    "        else:\n",
    "            color = text.capitalize()\n",
    "    \n",
    "    return color\n",
    "\n",
    "catalog['colors'] = catalog['colors'].apply(color_tag)\n",
    "catalog[['color_list', 'n_colors', 'colors']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:24.451135Z",
     "start_time": "2021-04-14T01:12:24.416961Z"
    }
   },
   "outputs": [],
   "source": [
    "# attach the three output columns to the original dataset\n",
    "for col in ['is_womens_clothing', 'product_category', 'colors']:\n",
    "    catalog_copy[col] = catalog[col]\n",
    "\n",
    "catalog_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T01:12:24.458431Z",
     "start_time": "2021-04-14T01:12:24.453442Z"
    }
   },
   "outputs": [],
   "source": [
    "# export to csv if needed\n",
    "# catalog_copy.to_csv('catalog_with_category_color_tag.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
