{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 (Due 6:29pm PST April 6th, 2021): Word Vectorization, Regex Practice, and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may work with **one other person on this assignment**. You may also work independently if you prefer.\n",
    "\n",
    "If you just want to be assigned someone to work with, message me on Slack and I will assign you a partner to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Using the **McDonalds Yelp Review CSV file**, **process the reviews**.\n",
    "This means you should think briefly about:\n",
    "* what stopwords to remove (should you add any custom stopwords to the set? Remove any stopwords?)\n",
    "* what regex cleaning you may need to perform (for example, are there different ways of saying `hamburger` that you need to account for?)\n",
    "* stemming/lemmatization (explain in your notebook why you used stemming versus lemmatization). \n",
    "\n",
    "Next, **count-vectorize the dataset**. Use the **`sklearn.feature_extraction.text.CountVectorizer`** examples from `Linear Algebra, Distance and Similarity (Completed).ipynb` and `Text Preprocessing Techniques (Completed).ipynb` (read the last section, `Vectorization Techniques`).\n",
    "\n",
    "I do not want redundant features - for instance, I do not want `hamburgers` and `hamburger` to be two distinct columns in your document-term matrix. Therefore, I'll be taking a look to make sure you've properly performed your cleaning, stopword removal, etc. to reduce the number of dimensions in your dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/yutongwanyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/yutongwanyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/yutongwanyan/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt') # A popular NLTK sentence tokenizer\n",
    "nltk.download('stopwords') # library of common English stopwords\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    _unit_id     city                                             review\n",
       "0  679455653  Atlanta  I'm not a huge mcds lover, but I've been to be...\n",
       "1  679455654  Atlanta  Terrible customer service. I came in at 9:30pm...\n",
       "2  679455655  Atlanta  First they \"lost\" my order, actually they gave...\n",
       "3  679455656  Atlanta  I see I'm not the only one giving 1 star. Only...\n",
       "4  679455657  Atlanta  Well, it's McDonald's, so you know what the fo..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_unit_id</th>\n      <th>city</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>679455653</td>\n      <td>Atlanta</td>\n      <td>I'm not a huge mcds lover, but I've been to be...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>679455654</td>\n      <td>Atlanta</td>\n      <td>Terrible customer service. I came in at 9:30pm...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>679455655</td>\n      <td>Atlanta</td>\n      <td>First they \"lost\" my order, actually they gave...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>679455656</td>\n      <td>Atlanta</td>\n      <td>I see I'm not the only one giving 1 star. Only...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>679455657</td>\n      <td>Atlanta</td>\n      <td>Well, it's McDonald's, so you know what the fo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "yelp = pd.read_csv('mcdonalds-yelp-negative-reviews.csv', encoding='latin-1')\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1525, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = yelp['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize & Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decide to use stemming because it's (1) working fast on a 1500+ rows series and (2) the goal of this practice is to get features with less redundants so I think we can be a bit aggressive there in terms of tekenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       I 'm not a huge mcd lover , but I 've been to ...\n",
       "1       terribl custom servic . I came in at 9:30pm an...\n",
       "2       first they `` lost '' my order , actual they g...\n",
       "3       I see I 'm not the onli one give 1 star . onli...\n",
       "4       well , it 's mcdonald 's , so you know what th...\n",
       "                              ...                        \n",
       "1520    I enjoy the part where I repeatedli ask if I h...\n",
       "1521    worst mcdonald I 've been in in a long time ! ...\n",
       "1522    when I am realli crave for mcdonald 's , thi s...\n",
       "1523    two point right out of the gate : 1 . thuggeri...\n",
       "1524    I want to grab breakfast one morn befor work s...\n",
       "Name: review, Length: 1525, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "reviews = reviews.apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "reviews = reviews.apply(lambda x: ' '.join(x))\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More regex cleanning \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hillo hiis is wy'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'(he|th)', 'hi','hello this is wy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more can be added if needed \n",
    "# here I only list a few examples: xxxburger, barbeque, chocol, coffee, frappuccino\n",
    "\n",
    "pattern_dict = {\n",
    "    r'\\b[a-z]+burger|burger\\b':'burger',\n",
    "    r'\\bbarbe+[cq]+[a-z]|bbq\\b':'barbeque',\n",
    "    r'\\bchocol|chocolateat|chocolateatt\\b':'chocolate',\n",
    "    r'\\bcoffe+[a-z]|coffee+[a-z]|coffees\\b':\"coffee\",\n",
    "    r'\\bfrap+[a-z]\\b':'frappuccino'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       I 'm not a huge mcd lover , but I 've been to ...\n",
       "1       terribl custom servic . I came in at 9:30pm an...\n",
       "2       first they `` lost '' my order , actual they g...\n",
       "3       I see I 'm not the onli one give 1 star . onli...\n",
       "4       well , it 's mcdonald 's , so you know what th...\n",
       "                              ...                        \n",
       "1520    I enjoy the part where I repeatedli ask if I h...\n",
       "1521    worst mcdonald I 've been in in a long time ! ...\n",
       "1522    when I am realli crave for mcdonald 's , thi s...\n",
       "1523    two point right out of the gate : 1 . thuggeri...\n",
       "1524    I want to grab breakfast one morn befor work s...\n",
       "Name: review, Length: 1525, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "for i in pattern_dict:\n",
    "    reviews = reviews.apply(lambda x: re.sub(i, pattern_dict[i], x))\n",
    "reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords and Get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorize the Documents\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(reviews) \n",
    "X = X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe shape was (1525, 6441)\nDataframe shape is now (1525, 6327)\n"
     ]
    }
   ],
   "source": [
    "corpus_df = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# iterate through the Pandas dataframe, and drop the columns that reflect stopwords:\n",
    "original_columns = corpus_df.columns # get existing columns\n",
    "\n",
    "to_drop_columns = set(original_columns).intersection(set(stopwords.words('english')+ ['00', '000'])) # get the list of words to drop\n",
    "print(f\"Dataframe shape was {corpus_df.shape}\")\n",
    "corpus_df.drop(columns=to_drop_columns, inplace=True)\n",
    "print(f\"Dataframe shape is now {corpus_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.head().T.to_csv(\"yelp_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. **Stopwords, Stemming, Lemmatization Practice**\n",
    "\n",
    "Using the `tale-of-two-cities.txt` file from Week 1:\n",
    "* Count-vectorize the corpus. Treat each sentence as a document.\n",
    "\n",
    "How many features (dimensions) do you get when you:\n",
    "* Perform **stemming** and then **count-vectorization**.\n",
    "* Perform **lemmatization** and then **count-vectorization**.\n",
    "* Perform **lemmatization**, remove **stopwords**, **remove punctuation**, and then perform **count-vectorization**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'  IT WAS the best of times, it was the worst of times, it was the\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tale = open('/Users/yutongwanyan/Desktop/dso-560-nlp-text-analytics-SPRING-2021/Week 1/tale-of-two-cities.txt','r')\n",
    "tale.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['age of wisdom, it was the age of foolishness, it was the epoch of',\n",
       " 'belief, it was the epoch of incredulity, it was the season of Light,',\n",
       " 'it was the season of Darkness, it was the spring of hope, it was the',\n",
       " 'winter of despair, we had everything before us, we had nothing',\n",
       " 'before us, we were all going direct to Heaven, we were all going']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "tale_lines = tale.readlines()\n",
    "text = []\n",
    "\n",
    "for i in tale_lines:\n",
    "    i = re.sub('\\n', '', i) # get rid of n\n",
    "    text.append(i)\n",
    "\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-vectorize the corpus. Treat each sentence as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.sent_tokenize(text) # need to tokenize first!\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "vector = vectorizer.transform(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe shape is now(7764, 9568)\nNumber of features: 9568\n"
     ]
    }
   ],
   "source": [
    "# from vector to pd dataframe\n",
    "corpus_df = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# get rid of stopwords\n",
    "original_columns = corpus_df.columns \n",
    "to_drop_columns = set(original_columns).intersection(set(stopwords.words('english'))) # get the list of words to drop\n",
    "corpus_df.drop(columns=to_drop_columns, inplace=True)\n",
    "print(f\"Dataframe shape is now{corpus_df.shape}\")\n",
    "print(f\"Number of features: {corpus_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How many features (dimensions) do you get when you:\n",
    "\n",
    "* Perform **stemming** and then **count-vectorization**.\n",
    "* Perform **lemmatization** and then **count-vectorization**.\n",
    "* Perform **lemmatization**, remove **stopwords**, **remove punctuation**, and then perform **count-vectorization**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a re-use function \n",
    "\n",
    "# reload text\n",
    "def load_text_list(textname):\n",
    "    the_file = open(textname, 'r')\n",
    "    file_lines = the_file.readlines()\n",
    "    \n",
    "    text = []\n",
    "    for i in file_lines:\n",
    "        i = re.sub('\\n', '', i) \n",
    "        text.append(i)\n",
    "\n",
    "    return text\n",
    "\n",
    "###\n",
    "\n",
    "def stem_or_lemma(input_text, method, remove_punctuation = False):\n",
    "\n",
    "    input_text = pd.Series(input_text)\n",
    "\n",
    "    if method == 'stem':\n",
    "        stemmer = nltk.stem.porter.PorterStemmer()\n",
    "        if remove_punctuation == True:\n",
    "            from nltk.tokenize import RegexpTokenizer\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            input_text = input_text.apply(lambda x: tokenizer.tokenize(x)).apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "        else:\n",
    "            input_text = input_text.apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "        input_text = input_text.apply(lambda x: ' '.join(x))\n",
    "    elif method == 'lemma':\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        if remove_punctuation == True:\n",
    "            from nltk.tokenize import RegexpTokenizer\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            input_text = input_text.apply(lambda x: tokenizer.tokenize(x)).apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "        else:\n",
    "            input_text = input_text.apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "        input_text = input_text.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return input_text\n",
    "\n",
    "###\n",
    "\n",
    "def count_vector_corpus(input_text, remove_stopwords = True):\n",
    "\n",
    "    #input_text = nltk.sent_tokenize(input_text) # tokenize \n",
    "\n",
    "    vectorizer = CountVectorizer() # count vectorize\n",
    "    vectorizer.fit(input_text)\n",
    "    vector = vectorizer.transform(input_text)\n",
    "\n",
    "    # from vector to pd dataframe\n",
    "    corpus_df = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "    # get rid of stopwords\n",
    "    if remove_stopwords == True:\n",
    "        original_columns = corpus_df.columns \n",
    "        to_drop_columns = set(original_columns).intersection(set(stopwords.words('english'))) \n",
    "        # get the list of words to drop\n",
    "        corpus_df.drop(columns=to_drop_columns, inplace=True)\n",
    "\n",
    "    print(f\"Dataframe shape is now {corpus_df.shape}\")\n",
    "    print(f\"Number of features: {corpus_df.shape[1]}\")\n",
    "\n",
    "    return corpus_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform stemming and then count-vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe shape is now (12870, 6659)\nNumber of features: 6659\n"
     ]
    }
   ],
   "source": [
    "text = load_text_list('/Users/yutongwanyan/Desktop/dso-560-nlp-text-analytics-SPRING-2021/Week 1/tale-of-two-cities.txt')\n",
    "stemmed_text = stem_or_lemma(text, 'stem', remove_punctuation = False)\n",
    "stemmed_corpus_df = count_vector_corpus(stemmed_text, remove_stopwords = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform **lemmatization** and then **count-vectorization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe shape is now (12870, 8910)\nNumber of features: 8910\n"
     ]
    }
   ],
   "source": [
    "text = load_text_list('/Users/yutongwanyan/Desktop/dso-560-nlp-text-analytics-SPRING-2021/Week 1/tale-of-two-cities.txt')\n",
    "stemmed_text = stem_or_lemma(text, 'lemma', remove_punctuation = False)\n",
    "stemmed_corpus_df = count_vector_corpus(stemmed_text, remove_stopwords = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform **lemmatization**, remove **stopwords**, **remove punctuation**, and then perform **count-vectorization**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# remove punctuation \n",
    "# https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe shape is now (12870, 8705)\nNumber of features: 8705\n"
     ]
    }
   ],
   "source": [
    "text = load_text_list('/Users/yutongwanyan/Desktop/dso-560-nlp-text-analytics-SPRING-2021/Week 1/tale-of-two-cities.txt')\n",
    "stemmed_text = stem_or_lemma(text, 'lemma', remove_punctuation = True)\n",
    "stemmed_corpus_df = count_vector_corpus(stemmed_text, remove_stopwords = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe shape is now (12870, 6273)\nNumber of features: 6273\n"
     ]
    }
   ],
   "source": [
    "# how about stemming, remove stopwords, remove punctuation, and then perform count-vectorization?\n",
    "\n",
    "text = load_text_list('/Users/yutongwanyan/Desktop/dso-560-nlp-text-analytics-SPRING-2021/Week 1/tale-of-two-cities.txt')\n",
    "stemmed_text = stem_or_lemma(text, 'stem', remove_punctuation = True)\n",
    "stemmed_corpus_df = count_vector_corpus(stemmed_text, remove_stopwords = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classwork For Lecture 2 (Due 6:29pm PST March 30th, 2021): Word Vectorization, Regex Practice, and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick A or B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Answer all the exercise questions in the **Text Preprocessing** notebook.\n",
    "\n",
    "B. Answer the below questions about text encoding and word count distributions:\n",
    "\n",
    "1. Which of the encodings below will be able to encode this text: 사업.\n",
    "* `ascii`\n",
    "* `latin1`\n",
    "* `utf-8`\n",
    "* `utf-32`\n",
    "* `extended ascii`\n",
    "\n",
    "2. **True or False**: the word dog will have the same binary representation regardless of whether it is ASCII, latin1, or utf8. If False, explain why it is false.\n",
    "\n",
    "\n",
    "3. According to the Zipf Law approximation, approximately what frequency (express it has a percent) would the 3rd most popular word in a generic piece of text appear?\n",
    "\n",
    "\n",
    "4. **True or False**: what is considered a stopword changes depending on the business context and dataset you are working with. If true, provide an example. If false, explain why it is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}