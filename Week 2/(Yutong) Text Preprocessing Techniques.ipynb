{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Information-Retrieval\" data-toc-modified-id=\"Information-Retrieval-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Information Retrieval</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Practical-Example\" data-toc-modified-id=\"Practical-Example-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Practical Example</a></span></li><li><span><a href=\"#Boolean-Search-Queries\" data-toc-modified-id=\"Boolean-Search-Queries-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Boolean Search Queries</a></span></li><li><span><a href=\"#Examples-of-Documents-That-Would-Satisfy-the-Above-Document-Term-Matrix\" data-toc-modified-id=\"Examples-of-Documents-That-Would-Satisfy-the-Above-Document-Term-Matrix-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Examples of Documents That Would Satisfy the Above Document-Term Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Query-Example-(nuclear-AND-treaty)\" data-toc-modified-id=\"Query-Example-(nuclear-AND-treaty)-2.0.3.1\"><span class=\"toc-item-num\">2.0.3.1&nbsp;&nbsp;</span>Query Example <code>(nuclear AND treaty)</code></a></span></li><li><span><a href=\"#Query-Example-(nuclear-AND-treaty)-OR-((NOT-treaty)-AND-(nonproliferation-OR-Iran))\" data-toc-modified-id=\"Query-Example-(nuclear-AND-treaty)-OR-((NOT-treaty)-AND-(nonproliferation-OR-Iran))-2.0.3.2\"><span class=\"toc-item-num\">2.0.3.2&nbsp;&nbsp;</span>Query Example <code>(nuclear AND treaty) OR ((NOT treaty) AND (nonproliferation OR Iran))</code></a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Tokenizing-Sentences\" data-toc-modified-id=\"Tokenizing-Sentences-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenizing Sentences</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition-of-a-Token\" data-toc-modified-id=\"Definition-of-a-Token-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Definition of a Token</a></span></li><li><span><a href=\"#Examples-of-Tokens/Words-Not-Being-the-Identical-Concepts\" data-toc-modified-id=\"Examples-of-Tokens/Words-Not-Being-the-Identical-Concepts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Examples of Tokens/Words Not Being the Identical Concepts</a></span></li><li><span><a href=\"#Definition-of-a-Document\" data-toc-modified-id=\"Definition-of-a-Document-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Definition of a Document</a></span></li><li><span><a href=\"#Why-Is-Tokenization-Hard?\" data-toc-modified-id=\"Why-Is-Tokenization-Hard?-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Why Is Tokenization Hard?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-Boundary-Detection\" data-toc-modified-id=\"Sentence-Boundary-Detection-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Sentence Boundary Detection</a></span></li><li><span><a href=\"#Approaches-to-Sentence-Boundary-Detection\" data-toc-modified-id=\"Approaches-to-Sentence-Boundary-Detection-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Approaches to Sentence Boundary Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Rules-Based\" data-toc-modified-id=\"Rules-Based-3.4.2.1\"><span class=\"toc-item-num\">3.4.2.1&nbsp;&nbsp;</span>Rules-Based</a></span></li><li><span><a href=\"#Supervised/Unsupervised-Learning\" data-toc-modified-id=\"Supervised/Unsupervised-Learning-3.4.2.2\"><span class=\"toc-item-num\">3.4.2.2&nbsp;&nbsp;</span>Supervised/Unsupervised Learning</a></span></li></ul></li><li><span><a href=\"#NLTK's-PunktSentenceTokenizer\" data-toc-modified-id=\"NLTK's-PunktSentenceTokenizer-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>NLTK's <code>PunktSentenceTokenizer</code></a></span></li></ul></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Lemmatization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-Stemming?\" data-toc-modified-id=\"Why-Stemming?-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Why Stemming?</a></span></li><li><span><a href=\"#Differences-Between-Stemming-and-Lemmatization\" data-toc-modified-id=\"Differences-Between-Stemming-and-Lemmatization-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Differences Between Stemming and Lemmatization</a></span></li></ul></li><li><span><a href=\"#Scoring-Metrics\" data-toc-modified-id=\"Scoring-Metrics-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Scoring Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Precision/Recall\" data-toc-modified-id=\"Precision/Recall-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Precision/Recall</a></span></li><li><span><a href=\"#F1-Score\" data-toc-modified-id=\"F1-Score-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>F1 Score</a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li><li><span><a href=\"#Removing-Stopwords\" data-toc-modified-id=\"Removing-Stopwords-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Removing Stopwords</a></span><ul class=\"toc-item\"><li><span><a href=\"#When-Should-You-Avoid-Removing-Stopwords?\" data-toc-modified-id=\"When-Should-You-Avoid-Removing-Stopwords?-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>When Should You Avoid Removing Stopwords?</a></span></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li><li><span><a href=\"#Vectorization-Techniques\" data-toc-modified-id=\"Vectorization-Techniques-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Vectorization Techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Count-Vectorization\" data-toc-modified-id=\"Count-Vectorization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Count Vectorization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download/install `nltk`, and then also download several of `nltk`'s modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/yutongwanyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/yutongwanyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # A popular NLTK sentence tokenizer\n",
    "nltk.download('stopwords') # library of common English stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Information retrieval (IR)** is finding material (usually documents) of an unstructured nature (usually text) that **satisfies an information need** from within large collections (usually stored on computers).\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "A mobile app contains a gallery of about **8,000** pieces of artwork that they'd like to sell to users. Each artwork contains artwork name, artist name, and some descriptions + keywords. Produce a search algorithm that will allow users to enter in a string text and return relevant pieces of artwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Search Queries\n",
    "\n",
    "Example of a **document-term matrix**, from [Northeastern University, Fall 2006 Information Retrieval](http://www.ccs.neu.edu/home/jaa/CSG339.06F/Lectures/boolean.pdf):\n",
    "\n",
    "<img src=\"images/boolean_search_query.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Documents That Would Satisfy the Above Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Document 1**\n",
    "\n",
    "\"The new cookbook was a huge hit with younger millenials.\"\n",
    "* **Document 2**\n",
    "\n",
    "\"Iran has significant nuclear production capabilities.\"\n",
    "* **Document 3**\n",
    "\n",
    "\"The treaty that ended the American Revolution was signed soon thereafter.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Example `(nuclear AND treaty)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`D7` would get returned since it is the only document that contains both `nuclear` and `treaty`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Example `(nuclear AND treaty) OR ((NOT treaty) AND (nonproliferation OR Iran))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `D7`: satisfies LHS (left-hand side of expression) - contains `nuclear` and `treaty`\n",
    "* `D5`: satisfied RHS - contains `nonproliferation` and not `treaty`)\n",
    "* `D2`: satisfies RHS - contains `Iran` and not `treaty`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Sentences\n",
    "\n",
    "## Definition of a Token\n",
    "\n",
    "> A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. ([Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)):\n",
    "\n",
    "<img src=\"images/tokenization.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "## Examples of Tokens/Words Not Being the Identical Concepts\n",
    "\n",
    "* Multi-word entities: `San Francisco`, `New York City`\n",
    "* Phone numbers / dates: `(800) 123-4564` and `Apr. 30th, 2020`\n",
    "\n",
    "\n",
    "At first, the task of tokenizing seems simple:\n",
    "1. Split apart `corpus` into `documents`.\n",
    "2. Split apart `documents` into `tokens`.\n",
    "\n",
    "## Definition of a Document\n",
    "\n",
    "A document is a distinct group of tokens. For example, in the Amazon toy product review dataset, each review can be considered a document. In *Tale of Two Cities*, a document could be either a sentence, or a paragraph, or a chapter - it all depends on the task at hand.\n",
    "\n",
    "* **Sentiment Analysis**: each document is a review/comment/tweet\n",
    "* **Authorship Classification** (determining if a piece of text was written by Person A or Person B): the entire text is a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Is Tokenization Hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Boundary Detection\n",
    "\n",
    "Deciding where one sentence ends and another sentence begins is easy for humans, but extremely complex for machines. A prime cause of this is ambiguity of punctuation marks. Particularly in English, periods can mean many things beyond denoting the end of a sentence:\n",
    "* abbreviations\n",
    "* decimal points\n",
    "* domain names and email addresses\n",
    "\n",
    "Furthermore, other \"obvious\" sentence boundaries like question marks and exclamation marks are becoming increasingly ambiguous due to slang, emoticons, or just emphasis (`\"What are you doing???\"` should not be split into 3 separate sentences).\n",
    "\n",
    "### Approaches to Sentence Boundary Detection\n",
    "\n",
    "#### Rules-Based\n",
    "\n",
    "* Using regex: `((?<=[a-z0-9][.?!])|(?<=[a-z0-9][.?!]\\\"))(\\s|\\r\\n)(?=\\\"?[A-Z])` (don't worry if you can't understand this yet- it includes usage of several advanced techniques such as **lookahead/behind references** which we have not covered).\n",
    "\n",
    "#### Supervised/Unsupervised Learning\n",
    "\n",
    "* Maximum entropy models ([UC Berkeley's SATZ Adaptive Sentence Boundary Detector](https://web.archive.org/web/20070922132340/http://elib.cs.berkeley.edu/src/satz/))\n",
    "* NLTK's `Punkt` sentence tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not just tokenize myself?\n",
    "text = \"I made two purchases today! I bought a bag of grapes for $4.99, \\\n",
    "but then... realized John Francis already bought some at the Y.M.C.A!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I made two purchases today! I bought a bag of grapes for $4',\n",
       " '99, but then',\n",
       " '',\n",
       " '',\n",
       " ' realized John Francis already bought some at the Y',\n",
       " 'M',\n",
       " 'C',\n",
       " 'A!']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# trying to write our own tokenizer\n",
    "text.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the issues you notice with the above approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK's `PunktSentenceTokenizer`\n",
    "\n",
    "> `PunktSentenceTokenizer` is an **sentence boundary detection algorithm** that must be trained to be used. NLTK already includes a pre-trained version of the `PunktSentenceTokenizer` ([StackOverflow](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I made two purchases today!',\n",
       " 'I bought a bag of grapes for $4.99, but then... realized John Francis already bought some at the Y.M.C.A!']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Using NLTK sent_tokenize()\n",
    "import nltk\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language [Source](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n",
    "\n",
    "<img src=\"images/stemming-examples.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "In Python, we can use **`nltk.stem.porter.PorterStemmer`** stem our words:\n",
    "\n",
    "```python\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"caressed\"))  # caress\n",
    "print(stemmer.stem(\"athlete\"))  # athlet\n",
    "print(stemmer.stem(\"athletics\"))  # athlet\n",
    "print(stemmer.stem(\"media\"))  # media\n",
    "print(stemmer.stem(\"photography\"))  # photographi\n",
    "print(stemmer.stem(\"sexy\"))  # sexi\n",
    "print(stemmer.stem(\"journalling\"))  # journal\n",
    "print(stemmer.stem(\"Slovakia\")) # slovakia\n",
    "print(stemmer.stem(\"corpora\")) # corpora\n",
    "print(stemmer.stem(\"thieves\")) # thiev\n",
    "print(stemmer.stem(\"rocks\")) # rock\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'rock'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('rocks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is closely related to stemming. However, while stemming looks only at the individual word itself, and considers the usage of the word (ie. part of speech, is this word a noun, a verb, etc.). For example, if we compared the [outputs of stemming and lemmatization certain ambiguous French words](https://blog.bitext.com/lemmatization-vs-stemming):\n",
    "\n",
    "<img src=\"images/comparison.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"caressed\")) #caressed\n",
    "print(lemmatizer.lemmatize(\"athlete\")) #athlete\n",
    "print(lemmatizer.lemmatize(\"athletics\")) #athletics\n",
    "print(lemmatizer.lemmatize(\"media\"))\n",
    "print(lemmatizer.lemmatize(\"photography\")) #photography\n",
    "print(lemmatizer.lemmatize(\"sexy\")) #sexy\n",
    "print(lemmatizer.lemmatize(\"journalling\")) #journalling\n",
    "print(lemmatizer.lemmatize(\"Slovakia\")) #Slovakia\n",
    "print(lemmatizer.lemmatize(\"corpora\")) # corpus\n",
    "print(lemmatizer.lemmatize(\"thieves\")) # thief\n",
    "print(lemmatizer.lemmatize(\"rocks\")) #rock\n",
    "```\n",
    "\n",
    "### Why Stemming?\n",
    "- smaller and faster\n",
    "- simplicity in \"good enough\"\n",
    "- can often **provide higher recall (coverage)** if you are using it for text searching: `drives` and `drivers` will likely shorten to `driv`, which may be useful if your search engine wants to make sure to get all relevant documents, even at the cost of surfacing a few irrelevant documents\n",
    "- could potentially be more useful for predictive models that tend to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences Between Stemming and Lemmatization\n",
    "> **Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token `saw`, stemming might return just `s`, whereas lemmatization would attempt to return either `see` or `saw` depending on whether the use of the token was as a verb or a noun. ([Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Metrics\n",
    "\n",
    "<img src=\"images/confusion_matrix2.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "### Precision/Recall\n",
    "\n",
    "**Recall:** What percent of the positive classes did the model successfully predict?\n",
    "\n",
    "**Precision:** When a model predicted a positive class, what percentage of the time was it correct?\n",
    "\n",
    "In terms of NLP / stemming / lemmatization:\n",
    "\n",
    "**Recall**: After processing (tokenizing, stemming/lemmatizing) the data, what percent of the relevant search results were surfaced? Ie. - when a user searches for `blue jeans`, did all the results returned include all the relevant items (blue-ish colored denim pants)?\n",
    "\n",
    "**Precision**: After processing (tokenizing, stemming/lemmatizing) the data, what percent of the results returned were relevant?\n",
    "\n",
    "<img src=\"images/matrix_practice3.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score of a model represents the harmonic mean between precision and recall, and is defined as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{1} = 2 * \\frac{P * R}{P + R}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "An F1 score is often a good measure \n",
    "* our dataset target is class imbalanced (ie. 96% positive, 4% negative)\n",
    "* when we want to balance optimizing for both precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Calculate:\n",
    "\n",
    "1. Overall **accuracy**: $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "(10+20)/36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Precision:** $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "10/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Recall:** $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "10/(10+4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "\n",
    "It's your call ultimately if you want to remove stopwords. There are advantages and disadvantages to both approaches. You will first need to run `nltk.download(\"stopwords\")` to download the set of stopwords for NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'o', 'didn', 'then', 'mightn', 'between', 'won', \"couldn't\", 'doing', 'than', 'can', 'if', 'they', 'these', 'until', 'does', 'haven', 'so', 'i', 'its', 'such', \"needn't\", 'now', 'them', 'wouldn', 'with', 'ain', 'into', 'that', 'only', 'aren', 'at', 'my', \"mightn't\", 'will', 'couldn', 'own', \"isn't\", 'yours', 'for', 'here', 'why', \"you'd\", 'nor', 'but', 're', 'below', 'their', 'where', \"won't\", 'under', 'ours', \"didn't\", 'above', 'hadn', 'as', 'having', 'off', 'during', 'what', 'each', \"aren't\", 'in', 'themselves', 'yourself', 'weren', \"you'll\", 'she', 'his', 'myself', 'don', 'to', 'again', 'not', 'too', 'because', \"you're\", 'be', 'further', 't', 'most', 'did', 'being', 'before', 'about', 'been', 'ourselves', 'had', 'or', 's', 've', 'while', 'ma', 'an', 'this', 'of', 'itself', 'some', \"wasn't\", \"it's\", 'on', 'through', \"weren't\", 'were', 'when', 'those', 'you', 'is', 'hasn', 'whom', \"wouldn't\", 'both', 'by', 'd', 'm', \"should've\", 'll', \"shouldn't\", 'wasn', 'there', 'out', \"don't\", 'should', 'all', 'same', \"you've\", 'her', \"hasn't\", 'mustn', 'shouldn', 'am', 'who', 'any', 'hers', 'once', 'was', 'over', 'we', 'it', 'and', 'just', \"mustn't\", 'are', 'me', 'down', \"hadn't\", 'isn', 'the', 'do', 'few', 'him', 'theirs', 'how', 'up', \"haven't\", 'against', 'from', \"doesn't\", \"that'll\", 'doesn', 'needn', 'has', 'very', 'shan', 'our', 'which', 'no', 'herself', \"shan't\", 'yourselves', 'a', 'y', 'other', 'himself', 'your', \"she's\", 'more', 'after', 'he', 'have'}\n"
     ]
    }
   ],
   "source": [
    "# see the set of words NLTK considers stopwords\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the Pandas dataframe, and drop the columns that reflect stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize(title):\n",
    "    if isinstance(title, str):\n",
    "        tokens = nltk.word_tokenize(title)\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if stopword in title:\n",
    "                title = title.replace(stopword, \"\")\n",
    "\n",
    "        return title\n",
    "    return None\n",
    "\n",
    "# in panda, nan/null considers as float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_art = pd.read_csv('baltimore_public_art.csv')\n",
    "\n",
    "\n",
    "\n",
    "public_art.titleOfArtWork.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Should You Avoid Removing Stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    (\"The product is really very good\", \"Positive\"),\n",
    "    (\"The products seems to be good\", \"Positive\"),\n",
    "    (\"Good product. I really liked it\", \"Positive\"),\n",
    "    (\"I didn’t like the product\", \"Negative\"),\n",
    "    (\"The product is not good.\", \"Negative\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cleaned_reviews = []\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# This iterates through each of the reviews, splitting the review into distinct tokens\n",
    "# Then it checks each token for whether or not it is a stopword, before adding them back into a \"cleaned_review\"\n",
    "for review in reviews:\n",
    "    words = review[0].split(\" \")\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in nltk_stopwords:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_review = \" \".join(new_words)\n",
    "    cleaned_reviews.append((cleaned_review, review[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The product really good', 'Positive'),\n",
       " ('The products seems good', 'Positive'),\n",
       " ('Good product. I really liked', 'Positive'),\n",
       " ('I didn’t like product', 'Negative'),\n",
       " ('The product good.', 'Negative')]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "cleaned_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singh (https://gaganmanku09.medium.com/why-you-should-avoid-removing-stopwords-4fe001d0f5b6):\n",
    "> If you are working with **basic NLP techniques like BOW, Count Vectorizer or TF-IDF(Term Frequency and Inverse Document Frequency)** then removing stopwords is a good idea because stopwords act like noise for these methods. If you working with **LSTMs or other models which capture the semantic meaning and the meaning of a word depends on the context of the previous text**, then it becomes important not to remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "##### 1. For each of the following statements, label them True or False. If False, briefly explain why:\n",
    "\n",
    "\n",
    "A. *If the **F1 score** of a model is **1.0 (100%)**, then the accuracy of your model must also be **100%**.*\n",
    "\n",
    "B. *Stemming **increases the size of the vocabulary** (the vocabulary is the set of all tokens found inside the corpus)*\n",
    "\n",
    "C. *Texts processed using lemmatization will typically have higher recall than stemming.*\n",
    "\n",
    "##### 2. Calculate precison and recall given the following results from a confusion matrix:\n",
    "\n",
    "<img src=\"images/exercise.jpeg\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "##### 3. Provide an example of how stemming can improve recall.\n",
    "\n",
    "##### 4. Provide an example of when stemming might reduce precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization\n",
    "<img src=\"images/count_vectorizer.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use **`sklearn.feature_extraction.text.CountVectorizer`** to easily convert your corpus into a bag of words matrix:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games. Mary does not like football much.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "```\n",
    "Note that the output `X` here is not your traditional Numpy matrix! Calling **`type(X)`** here will yield **`<class 'scipy.sparse.csr.csr_matrix'>`**, which is a **CSR ([compressed sparse row format matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html))**. To convert it into an actual matrix, call the `toarray()` method:\n",
    "\n",
    "```python\n",
    "X.toarray()\n",
    "```\n",
    "Your output will be \n",
    "\n",
    "```\n",
    "array([[0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 1],\n",
    "       [1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1]], dtype=int64)\n",
    "```\n",
    "Notice that using **`X.shape`** $\\rightarrow$ `(2,14)`, indicating a total vocabulary size $V$ of 14. To get what word each of the 14 columns corresponds to, use **`vectorizer.get_feature_names()`**:\n",
    "```\n",
    "['also', 'does', 'football', 'games', 'john', 'like', 'likes', 'mary', 'movies', 'much', 'not', 'to', 'too', 'watch']\n",
    "```\n",
    "\n",
    "Notice, however, that as the vocabulary size $V$ increases, the percent of the matrix taken up by zero values increases:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits.\"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"It's still early, so box-office disappointments are still among the highest-grossing movies of the year.\", \n",
    "        \"That movie was terrific\", \"You love cats\", \n",
    "        \"Pay for top executives at big US companies is vastly higher than what everyday workers make, and a new report from The Wall Street Journal has found that CEOs have hit an eye-popping milestone in the size of their monthly paychecks.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the outputted vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vectorized corpus into Pandas dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}