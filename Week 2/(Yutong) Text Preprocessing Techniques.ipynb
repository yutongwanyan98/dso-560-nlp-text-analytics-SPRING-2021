{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Information-Retrieval\" data-toc-modified-id=\"Information-Retrieval-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Information Retrieval</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Practical-Example\" data-toc-modified-id=\"Practical-Example-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Practical Example</a></span></li><li><span><a href=\"#Boolean-Search-Queries\" data-toc-modified-id=\"Boolean-Search-Queries-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Boolean Search Queries</a></span></li><li><span><a href=\"#Examples-of-Documents-That-Would-Satisfy-the-Above-Document-Term-Matrix\" data-toc-modified-id=\"Examples-of-Documents-That-Would-Satisfy-the-Above-Document-Term-Matrix-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Examples of Documents That Would Satisfy the Above Document-Term Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Query-Example-(nuclear-AND-treaty)\" data-toc-modified-id=\"Query-Example-(nuclear-AND-treaty)-2.0.3.1\"><span class=\"toc-item-num\">2.0.3.1&nbsp;&nbsp;</span>Query Example <code>(nuclear AND treaty)</code></a></span></li><li><span><a href=\"#Query-Example-(nuclear-AND-treaty)-OR-((NOT-treaty)-AND-(nonproliferation-OR-Iran))\" data-toc-modified-id=\"Query-Example-(nuclear-AND-treaty)-OR-((NOT-treaty)-AND-(nonproliferation-OR-Iran))-2.0.3.2\"><span class=\"toc-item-num\">2.0.3.2&nbsp;&nbsp;</span>Query Example <code>(nuclear AND treaty) OR ((NOT treaty) AND (nonproliferation OR Iran))</code></a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Tokenizing-Sentences\" data-toc-modified-id=\"Tokenizing-Sentences-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenizing Sentences</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition-of-a-Token\" data-toc-modified-id=\"Definition-of-a-Token-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Definition of a Token</a></span></li><li><span><a href=\"#Examples-of-Tokens/Words-Not-Being-the-Identical-Concepts\" data-toc-modified-id=\"Examples-of-Tokens/Words-Not-Being-the-Identical-Concepts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Examples of Tokens/Words Not Being the Identical Concepts</a></span></li><li><span><a href=\"#Definition-of-a-Document\" data-toc-modified-id=\"Definition-of-a-Document-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Definition of a Document</a></span></li><li><span><a href=\"#Why-Is-Tokenization-Hard?\" data-toc-modified-id=\"Why-Is-Tokenization-Hard?-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Why Is Tokenization Hard?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-Boundary-Detection\" data-toc-modified-id=\"Sentence-Boundary-Detection-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Sentence Boundary Detection</a></span></li><li><span><a href=\"#Approaches-to-Sentence-Boundary-Detection\" data-toc-modified-id=\"Approaches-to-Sentence-Boundary-Detection-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Approaches to Sentence Boundary Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Rules-Based\" data-toc-modified-id=\"Rules-Based-3.4.2.1\"><span class=\"toc-item-num\">3.4.2.1&nbsp;&nbsp;</span>Rules-Based</a></span></li><li><span><a href=\"#Supervised/Unsupervised-Learning\" data-toc-modified-id=\"Supervised/Unsupervised-Learning-3.4.2.2\"><span class=\"toc-item-num\">3.4.2.2&nbsp;&nbsp;</span>Supervised/Unsupervised Learning</a></span></li></ul></li><li><span><a href=\"#NLTK's-PunktSentenceTokenizer\" data-toc-modified-id=\"NLTK's-PunktSentenceTokenizer-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>NLTK's <code>PunktSentenceTokenizer</code></a></span></li></ul></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Lemmatization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-Stemming?\" data-toc-modified-id=\"Why-Stemming?-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Why Stemming?</a></span></li><li><span><a href=\"#Differences-Between-Stemming-and-Lemmatization\" data-toc-modified-id=\"Differences-Between-Stemming-and-Lemmatization-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Differences Between Stemming and Lemmatization</a></span></li></ul></li><li><span><a href=\"#Scoring-Metrics\" data-toc-modified-id=\"Scoring-Metrics-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Scoring Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Precision/Recall\" data-toc-modified-id=\"Precision/Recall-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Precision/Recall</a></span></li><li><span><a href=\"#F1-Score\" data-toc-modified-id=\"F1-Score-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>F1 Score</a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li><li><span><a href=\"#Removing-Stopwords\" data-toc-modified-id=\"Removing-Stopwords-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Removing Stopwords</a></span><ul class=\"toc-item\"><li><span><a href=\"#When-Should-You-Avoid-Removing-Stopwords?\" data-toc-modified-id=\"When-Should-You-Avoid-Removing-Stopwords?-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>When Should You Avoid Removing Stopwords?</a></span></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li><li><span><a href=\"#Vectorization-Techniques\" data-toc-modified-id=\"Vectorization-Techniques-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Vectorization Techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Count-Vectorization\" data-toc-modified-id=\"Count-Vectorization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Count Vectorization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download/install `nltk`, and then also download several of `nltk`'s modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # A popular NLTK sentence tokenizer\n",
    "nltk.download('stopwords') # library of common English stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Information retrieval (IR)** is finding material (usually documents) of an unstructured nature (usually text) that **satisfies an information need** from within large collections (usually stored on computers).\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "A mobile app contains a gallery of about **8,000** pieces of artwork that they'd like to sell to users. Each artwork contains artwork name, artist name, and some descriptions + keywords. Produce a search algorithm that will allow users to enter in a string text and return relevant pieces of artwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Search Queries\n",
    "\n",
    "Example of a **document-term matrix**, from [Northeastern University, Fall 2006 Information Retrieval](http://www.ccs.neu.edu/home/jaa/CSG339.06F/Lectures/boolean.pdf):\n",
    "\n",
    "<img src=\"images/boolean_search_query.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Documents That Would Satisfy the Above Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Document 1**\n",
    "\n",
    "\"The new cookbook was a huge hit with younger millenials.\"\n",
    "* **Document 2**\n",
    "\n",
    "\"Iran has significant nuclear production capabilities.\"\n",
    "* **Document 3**\n",
    "\n",
    "\"The treaty that ended the American Revolution was signed soon thereafter.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Example `(nuclear AND treaty)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`D7` would get returned since it is the only document that contains both `nuclear` and `treaty`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Example `(nuclear AND treaty) OR ((NOT treaty) AND (nonproliferation OR Iran))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `D7`: satisfies LHS (left-hand side of expression) - contains `nuclear` and `treaty`\n",
    "* `D5`: satisfied RHS - contains `nonproliferation` and not `treaty`)\n",
    "* `D2`: satisfies RHS - contains `Iran` and not `treaty`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Sentences\n",
    "\n",
    "## Definition of a Token\n",
    "\n",
    "> A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. ([Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)):\n",
    "\n",
    "<img src=\"images/tokenization.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "## Examples of Tokens/Words Not Being the Identical Concepts\n",
    "\n",
    "* Multi-word entities: `San Francisco`, `New York City`\n",
    "* Phone numbers / dates: `(800) 123-4564` and `Apr. 30th, 2020`\n",
    "\n",
    "\n",
    "At first, the task of tokenizing seems simple:\n",
    "1. Split apart `corpus` into `documents`.\n",
    "2. Split apart `documents` into `tokens`.\n",
    "\n",
    "## Definition of a Document\n",
    "\n",
    "A document is a distinct group of tokens. For example, in the Amazon toy product review dataset, each review can be considered a document. In *Tale of Two Cities*, a document could be either a sentence, or a paragraph, or a chapter - it all depends on the task at hand.\n",
    "\n",
    "* **Sentiment Analysis**: each document is a review/comment/tweet\n",
    "* **Authorship Classification** (determining if a piece of text was written by Person A or Person B): the entire text is a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Is Tokenization Hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Boundary Detection\n",
    "\n",
    "Deciding where one sentence ends and another sentence begins is easy for humans, but extremely complex for machines. A prime cause of this is ambiguity of punctuation marks. Particularly in English, periods can mean many things beyond denoting the end of a sentence:\n",
    "* abbreviations\n",
    "* decimal points\n",
    "* domain names and email addresses\n",
    "\n",
    "Furthermore, other \"obvious\" sentence boundaries like question marks and exclamation marks are becoming increasingly ambiguous due to slang, emoticons, or just emphasis (`\"What are you doing???\"` should not be split into 3 separate sentences).\n",
    "\n",
    "### Approaches to Sentence Boundary Detection\n",
    "\n",
    "#### Rules-Based\n",
    "\n",
    "* Using regex: `((?<=[a-z0-9][.?!])|(?<=[a-z0-9][.?!]\\\"))(\\s|\\r\\n)(?=\\\"?[A-Z])` (don't worry if you can't understand this yet- it includes usage of several advanced techniques such as **lookahead/behind references** which we have not covered).\n",
    "\n",
    "#### Supervised/Unsupervised Learning\n",
    "\n",
    "* Maximum entropy models ([UC Berkeley's SATZ Adaptive Sentence Boundary Detector](https://web.archive.org/web/20070922132340/http://elib.cs.berkeley.edu/src/satz/))\n",
    "* NLTK's `Punkt` sentence tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not just tokenize myself?\n",
    "text = \"I made two purchases today! I bought a bag of grapes for $4.99, \\\n",
    "but then... realized John Francis already bought some at the Y.M.C.A!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made two purchases today! I bought a bag of grapes for $4',\n",
       " '99, but then',\n",
       " '',\n",
       " '',\n",
       " ' realized John Francis already bought some at the Y',\n",
       " 'M',\n",
       " 'C',\n",
       " 'A!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to write our own tokenizer\n",
    "text.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the issues you notice with the above approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK's `PunktSentenceTokenizer`\n",
    "\n",
    "> `PunktSentenceTokenizer` is an **sentence boundary detection algorithm** that must be trained to be used. NLTK already includes a pre-trained version of the `PunktSentenceTokenizer` ([StackOverflow](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made two purchases today!',\n",
       " 'I bought a bag of grapes for $4.99, but then... realized John Francis already bought some at the Y.M.C.A!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using NLTK sent_tokenize()\n",
    "import nltk\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language [Source](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n",
    "\n",
    "<img src=\"images/stemming-examples.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "In Python, we can use **`nltk.stem.porter.PorterStemmer`** stem our words:\n",
    "\n",
    "```python\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"caressed\"))  # caress\n",
    "print(stemmer.stem(\"athlete\"))  # athlet\n",
    "print(stemmer.stem(\"athletics\"))  # athlet\n",
    "print(stemmer.stem(\"media\"))  # media\n",
    "print(stemmer.stem(\"photography\"))  # photographi\n",
    "print(stemmer.stem(\"sexy\"))  # sexi\n",
    "print(stemmer.stem(\"journalling\"))  # journal\n",
    "print(stemmer.stem(\"Slovakia\")) # slovakia\n",
    "print(stemmer.stem(\"corpora\")) # corpora\n",
    "print(stemmer.stem(\"thieves\")) # thiev\n",
    "print(stemmer.stem(\"rocks\")) # rock\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b-day'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"b-day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is closely related to stemming. However, while stemming looks only at the individual word itself, and considers the usage of the word (ie. part of speech, is this word a noun, a verb, etc.). For example, if we compared the [outputs of stemming and lemmatization certain ambiguous French words](https://blog.bitext.com/lemmatization-vs-stemming):\n",
    "\n",
    "<img src=\"images/comparison.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"caressed\")) #caressed\n",
    "print(lemmatizer.lemmatize(\"athlete\")) #athlete\n",
    "print(lemmatizer.lemmatize(\"athletics\")) #athletics\n",
    "print(lemmatizer.lemmatize(\"media\"))\n",
    "print(lemmatizer.lemmatize(\"photography\")) #photography\n",
    "print(lemmatizer.lemmatize(\"sexy\")) #sexy\n",
    "print(lemmatizer.lemmatize(\"journalling\")) #journalling\n",
    "print(lemmatizer.lemmatize(\"Slovakia\")) #Slovakia\n",
    "print(lemmatizer.lemmatize(\"corpora\")) # corpus\n",
    "print(lemmatizer.lemmatize(\"thieves\")) # thief\n",
    "print(lemmatizer.lemmatize(\"rocks\")) #rock\n",
    "```\n",
    "\n",
    "### Why Stemming?\n",
    "- smaller and faster\n",
    "- simplicity in \"good enough\"\n",
    "- can often **provide higher recall (coverage)** if you are using it for text searching: `drives` and `drivers` will likely shorten to `driv`, which may be useful if your search engine wants to make sure to get all relevant documents, even at the cost of surfacing a few irrelevant documents\n",
    "- could potentially be more useful for predictive models that tend to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences Between Stemming and Lemmatization\n",
    "> **Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token `saw`, stemming might return just `s`, whereas lemmatization would attempt to return either `see` or `saw` depending on whether the use of the token was as a verb or a noun. ([Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Metrics\n",
    "\n",
    "<img src=\"images/confusion_matrix2.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "### Precision/Recall\n",
    "\n",
    "**Recall:** What percent of the positive classes did the model successfully predict?\n",
    "\n",
    "**Precision:** When a model predicted a positive class, what percentage of the time was it correct?\n",
    "\n",
    "In terms of NLP / stemming / lemmatization:\n",
    "\n",
    "**Recall**: After processing (tokenizing, stemming/lemmatizing) the data, what percent of the relevant search results were surfaced? Ie. - when a user searches for `blue jeans`, did all the results returned include all the relevant items (blue-ish colored denim pants)?\n",
    "\n",
    "**Precision**: After processing (tokenizing, stemming/lemmatizing) the data, what percent of the results returned were relevant?\n",
    "\n",
    "<img src=\"images/matrix_practice3.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score of a model represents the harmonic mean between precision and recall, and is defined as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{1} = 2 * \\frac{P * R}{P + R}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "An F1 score is often a good measure \n",
    "* our dataset target is class imbalanced (ie. 96% positive, 4% negative)\n",
    "* when we want to balance optimizing for both precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Calculate:\n",
    "\n",
    "1. Overall **accuracy**: $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30 / 36 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TP + TN / total number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Precision:** $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / 12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually relevant items from the positive recommedantions / # of positive recommendations (things the model has tagged as relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Recall:** $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "\n",
    "It's your call ultimately if you want to remove stopwords. There are advantages and disadvantages to both approaches. You will first need to run `nltk.download(\"stopwords\")` to download the set of stopwords for NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'their', 'yours', 'as', 'some', 'ain', 'you', 'a', 'more', 'too', 'him', 'again', 'isn', \"wasn't\", 'wouldn', 'very', 'of', \"shan't\", \"should've\", 'all', 'after', 'its', 'was', 'off', 'shouldn', 'weren', 'for', 'and', 'is', 'did', 'during', 'below', 'm', 'in', 'had', 'so', 'can', 'd', 'through', 'those', 'am', 'we', 'to', 'won', 'being', \"wouldn't\", 'itself', 'theirs', 'y', \"weren't\", \"won't\", 'few', 'ourselves', 'the', \"mightn't\", \"don't\", 'same', \"you've\", 'that', 'have', 'your', 'above', 'his', 'further', 'not', \"didn't\", \"that'll\", 'ma', 'ours', \"needn't\", 'our', 'there', 'between', 'on', 'should', \"you'd\", 'most', 'were', 'up', 'down', 'aren', 'having', 'shan', 'will', 'before', 'how', \"couldn't\", 'them', 'it', 'because', 'both', 'at', 'yourself', 'out', 'by', 'be', 'himself', 'with', \"shouldn't\", \"aren't\", 'do', 'about', 'or', 'only', 'when', 'once', 'why', 've', 'until', 'whom', 'needn', 'couldn', 'which', 'than', 'll', \"you'll\", 'such', \"mustn't\", 'me', 'an', 'hers', 'doing', 'he', \"hadn't\", 'against', \"hasn't\", 'o', 'are', 'myself', 'here', 'under', 'herself', 'while', \"haven't\", 'other', 'they', 'has', \"she's\", 'nor', 'now', 't', 'i', 'yourselves', 'does', 'over', 'into', 'mightn', 'wasn', 'been', 'doesn', 'if', 'from', 'each', 'hasn', \"you're\", 'this', 're', 'what', 'own', \"isn't\", 'hadn', 'themselves', 'mustn', 'who', 'didn', 'no', \"it's\", 'where', 'haven', 's', 'just', \"doesn't\", 'don', 'these', 'then', 'but', 'any', 'she', 'her', 'my'}\n"
     ]
    }
   ],
   "source": [
    "# see the set of words NLTK considers stopwords\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the Pandas dataframe, and drop the columns that reflect stopwords\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def tokenize(title):\n",
    "    \n",
    "    if isinstance(title, str):\n",
    "        tokens = nltk.word_tokenize(title)\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in english_stopwords:\n",
    "                continue\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "        return \" \".join(filtered_tokens)\n",
    "\n",
    "public_art_df = pd.read_csv(\"baltimore_public_art.csv\")\n",
    "public_art_df[\"titleOfArtwork_without_stopwords\"] = public_art_df[\"titleOfArtwork\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artistLastName</th>\n",
       "      <th>artistFirstName</th>\n",
       "      <th>image</th>\n",
       "      <th>titleOfArtwork</th>\n",
       "      <th>dateOfArtwork</th>\n",
       "      <th>medium</th>\n",
       "      <th>zipcodes</th>\n",
       "      <th>locationOfArtwork</th>\n",
       "      <th>addressOfArtwork</th>\n",
       "      <th>publicSchoolNumber</th>\n",
       "      <th>siteOfArtwork</th>\n",
       "      <th>descriptionOfArtwork</th>\n",
       "      <th>visibility</th>\n",
       "      <th>indoorOutdoorAccessible</th>\n",
       "      <th>monument</th>\n",
       "      <th>location</th>\n",
       "      <th>titleOfArtwork_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berge</td>\n",
       "      <td>Henry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Family Group</td>\n",
       "      <td>1953</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lafayette Courts</td>\n",
       "      <td>Lafayette Courts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lafayette Courts\\n</td>\n",
       "      <td>Family Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capellano</td>\n",
       "      <td>Antonio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moses and Christ Reliefs</td>\n",
       "      <td>c.1817</td>\n",
       "      <td>marble</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St. Paul's Episcopal Church</td>\n",
       "      <td>St. Paul's Episcopal Church</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reliefs on front of building</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St. Paul's Episcopal Church\\n</td>\n",
       "      <td>Moses Christ Reliefs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>Rodney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>William Donald Schaefer</td>\n",
       "      <td>2009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>William Donald Schaefer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cea</td>\n",
       "      <td>Dominic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlantic Blue Roller Column or Interlocking Pi...</td>\n",
       "      <td>1977</td>\n",
       "      <td>painted steel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russell St. Median strip</td>\n",
       "      <td>Russell St.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Median strip near Oriole Park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russell St.\\n</td>\n",
       "      <td>Atlantic Blue Roller Column Interlocking Piece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christie</td>\n",
       "      <td>Alden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baltimore Pylon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baltimore Washington Expressway</td>\n",
       "      <td>Baltimore Washington Expressway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>At city line</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baltimore Washington Expressway\\n</td>\n",
       "      <td>Baltimore Pylon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>Edmister</td>\n",
       "      <td>Stan</td>\n",
       "      <td>https://data.baltimorecity.gov/views/5xsg-uc29...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005</td>\n",
       "      <td>bridge repainting</td>\n",
       "      <td>21218</td>\n",
       "      <td>Howard St. Bridge over Jones Falls Expressway</td>\n",
       "      <td>Howard St. Bridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Howard St. Bridge over JFX</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>Very visible from street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Howard St. Bridge\\n21218\\n(39.32947000000013, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>Hess</td>\n",
       "      <td>David</td>\n",
       "      <td>https://data.baltimorecity.gov/views/5xsg-uc29...</td>\n",
       "      <td>Birds Nest Balcony</td>\n",
       "      <td>2004</td>\n",
       "      <td>metalic sculpture</td>\n",
       "      <td>21230</td>\n",
       "      <td>American Visionary Art Museum Tall Sculpture Barn</td>\n",
       "      <td>800 Key Highway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American Visionary Art Museum's James Rouse Bldg.</td>\n",
       "      <td>Massive sized bird's nest</td>\n",
       "      <td>Visible from sidewalk; in between buildings</td>\n",
       "      <td>Outdoor Accessible</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800 Key Highway\\n21230\\n(39.28022140477276, -7...</td>\n",
       "      <td>Birds Nest Balcony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>Borofsky</td>\n",
       "      <td>Jonathan</td>\n",
       "      <td>https://data.baltimorecity.gov/views/5xsg-uc29...</td>\n",
       "      <td>Male/ Female</td>\n",
       "      <td>2004</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>21201</td>\n",
       "      <td>Penn Station</td>\n",
       "      <td>1500 N. Charles St.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In front of Penn Station; between Charles and ...</td>\n",
       "      <td>Man and woman intersected</td>\n",
       "      <td>Very visible from street; in front of Penn Sta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1500 N. Charles St.\\n21201\\n(39.30711915495539...</td>\n",
       "      <td>Male/ Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>Honeycutt</td>\n",
       "      <td>Brece</td>\n",
       "      <td>https://data.baltimorecity.gov/views/5xsg-uc29...</td>\n",
       "      <td>silence</td>\n",
       "      <td>2002</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>21210</td>\n",
       "      <td>Bryn Mawr School</td>\n",
       "      <td>109 W. Melrose St.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Located in courtyard by Margaret Hamilton Bldg...</td>\n",
       "      <td>Seven lecturns with binders</td>\n",
       "      <td>Limited visibility; located on school grounds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109 W. Melrose St.\\n21210\\n(39.36557610689288,...</td>\n",
       "      <td>silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>Hess</td>\n",
       "      <td>David</td>\n",
       "      <td>https://data.baltimorecity.gov/views/5xsg-uc29...</td>\n",
       "      <td>Inertia Study</td>\n",
       "      <td>2002</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>21218</td>\n",
       "      <td>Mergenthaler Vocational HS</td>\n",
       "      <td>3500 Hillen Rd.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mergenthaler Vocation H.S. entrance</td>\n",
       "      <td>Five large circular objects with hole; looks l...</td>\n",
       "      <td>Visible; located on school grounds but, becaus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3500 Hillen Rd.\\n21218\\n(39.33167161806898, -7...</td>\n",
       "      <td>Inertia Study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    artistLastName artistFirstName  \\\n",
       "0            Berge           Henry   \n",
       "1        Capellano         Antonio   \n",
       "2          Carroll          Rodney   \n",
       "3              Cea         Dominic   \n",
       "4         Christie           Alden   \n",
       "..             ...             ...   \n",
       "685       Edmister            Stan   \n",
       "686           Hess           David   \n",
       "687       Borofsky        Jonathan   \n",
       "688      Honeycutt           Brece   \n",
       "689           Hess           David   \n",
       "\n",
       "                                                 image  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "685  https://data.baltimorecity.gov/views/5xsg-uc29...   \n",
       "686  https://data.baltimorecity.gov/views/5xsg-uc29...   \n",
       "687  https://data.baltimorecity.gov/views/5xsg-uc29...   \n",
       "688  https://data.baltimorecity.gov/views/5xsg-uc29...   \n",
       "689  https://data.baltimorecity.gov/views/5xsg-uc29...   \n",
       "\n",
       "                                        titleOfArtwork dateOfArtwork  \\\n",
       "0                                         Family Group          1953   \n",
       "1                             Moses and Christ Reliefs        c.1817   \n",
       "2                              William Donald Schaefer          2009   \n",
       "3    Atlantic Blue Roller Column or Interlocking Pi...          1977   \n",
       "4                                      Baltimore Pylon           NaN   \n",
       "..                                                 ...           ...   \n",
       "685                                                NaN          2005   \n",
       "686                                 Birds Nest Balcony          2004   \n",
       "687                                       Male/ Female          2004   \n",
       "688                                            silence          2002   \n",
       "689                                      Inertia Study          2002   \n",
       "\n",
       "                medium zipcodes  \\\n",
       "0             concrete      NaN   \n",
       "1               marble      NaN   \n",
       "2                  NaN      NaN   \n",
       "3        painted steel      NaN   \n",
       "4                  NaN      NaN   \n",
       "..                 ...      ...   \n",
       "685  bridge repainting    21218   \n",
       "686  metalic sculpture    21230   \n",
       "687          sculpture    21201   \n",
       "688          sculpture    21210   \n",
       "689          sculpture    21218   \n",
       "\n",
       "                                     locationOfArtwork  \\\n",
       "0                                     Lafayette Courts   \n",
       "1                          St. Paul's Episcopal Church   \n",
       "2                                                  NaN   \n",
       "3                             Russell St. Median strip   \n",
       "4                      Baltimore Washington Expressway   \n",
       "..                                                 ...   \n",
       "685      Howard St. Bridge over Jones Falls Expressway   \n",
       "686  American Visionary Art Museum Tall Sculpture Barn   \n",
       "687                                       Penn Station   \n",
       "688                                   Bryn Mawr School   \n",
       "689                         Mergenthaler Vocational HS   \n",
       "\n",
       "                    addressOfArtwork publicSchoolNumber  \\\n",
       "0                   Lafayette Courts                NaN   \n",
       "1        St. Paul's Episcopal Church                NaN   \n",
       "2                                NaN                NaN   \n",
       "3                        Russell St.                NaN   \n",
       "4    Baltimore Washington Expressway                NaN   \n",
       "..                               ...                ...   \n",
       "685                Howard St. Bridge                NaN   \n",
       "686                  800 Key Highway                NaN   \n",
       "687              1500 N. Charles St.                NaN   \n",
       "688               109 W. Melrose St.                NaN   \n",
       "689                  3500 Hillen Rd.                NaN   \n",
       "\n",
       "                                         siteOfArtwork  \\\n",
       "0                                                  NaN   \n",
       "1                         Reliefs on front of building   \n",
       "2                                                  NaN   \n",
       "3                        Median strip near Oriole Park   \n",
       "4                                         At city line   \n",
       "..                                                 ...   \n",
       "685                         Howard St. Bridge over JFX   \n",
       "686  American Visionary Art Museum's James Rouse Bldg.   \n",
       "687  In front of Penn Station; between Charles and ...   \n",
       "688  Located in courtyard by Margaret Hamilton Bldg...   \n",
       "689                Mergenthaler Vocation H.S. entrance   \n",
       "\n",
       "                                  descriptionOfArtwork  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "685                                             Bridge   \n",
       "686                          Massive sized bird's nest   \n",
       "687                          Man and woman intersected   \n",
       "688                        Seven lecturns with binders   \n",
       "689  Five large circular objects with hole; looks l...   \n",
       "\n",
       "                                            visibility  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "685                           Very visible from street   \n",
       "686        Visible from sidewalk; in between buildings   \n",
       "687  Very visible from street; in front of Penn Sta...   \n",
       "688      Limited visibility; located on school grounds   \n",
       "689  Visible; located on school grounds but, becaus...   \n",
       "\n",
       "    indoorOutdoorAccessible monument  \\\n",
       "0                       NaN      NaN   \n",
       "1                       NaN      NaN   \n",
       "2                       NaN      NaN   \n",
       "3                       NaN      NaN   \n",
       "4                       NaN      NaN   \n",
       "..                      ...      ...   \n",
       "685                     NaN      NaN   \n",
       "686      Outdoor Accessible      NaN   \n",
       "687                     NaN      NaN   \n",
       "688                     NaN      NaN   \n",
       "689                     NaN      NaN   \n",
       "\n",
       "                                              location  \\\n",
       "0                                   Lafayette Courts\\n   \n",
       "1                        St. Paul's Episcopal Church\\n   \n",
       "2                                                  NaN   \n",
       "3                                        Russell St.\\n   \n",
       "4                    Baltimore Washington Expressway\\n   \n",
       "..                                                 ...   \n",
       "685  Howard St. Bridge\\n21218\\n(39.32947000000013, ...   \n",
       "686  800 Key Highway\\n21230\\n(39.28022140477276, -7...   \n",
       "687  1500 N. Charles St.\\n21201\\n(39.30711915495539...   \n",
       "688  109 W. Melrose St.\\n21210\\n(39.36557610689288,...   \n",
       "689  3500 Hillen Rd.\\n21218\\n(39.33167161806898, -7...   \n",
       "\n",
       "                   titleOfArtwork_without_stopwords  \n",
       "0                                      Family Group  \n",
       "1                              Moses Christ Reliefs  \n",
       "2                           William Donald Schaefer  \n",
       "3    Atlantic Blue Roller Column Interlocking Piece  \n",
       "4                                   Baltimore Pylon  \n",
       "..                                              ...  \n",
       "685                                            None  \n",
       "686                              Birds Nest Balcony  \n",
       "687                                    Male/ Female  \n",
       "688                                         silence  \n",
       "689                                   Inertia Study  \n",
       "\n",
       "[690 rows x 17 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_art_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Should You Avoid Removing Stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    (\"The product is really very good\", \"Positive\"),\n",
    "    (\"The products seems to be good\", \"Positive\"),\n",
    "    (\"Good product. I really liked it\", \"Positive\"),\n",
    "    (\"I didn’t like the product\", \"Negative\"),\n",
    "    (\"The product is not good.\", \"Negative\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cleaned_reviews = []\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# This iterates through each of the reviews, splitting the review into distinct tokens\n",
    "# Then it checks each token for whether or not it is a stopword, before adding them back into a \"cleaned_review\"\n",
    "for review in reviews:\n",
    "    words = review[0].split(\" \")\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in nltk_stopwords:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_review = \" \".join(new_words)\n",
    "    cleaned_reviews.append((cleaned_review, review[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The product really good', 'Positive'),\n",
       " ('The products seems good', 'Positive'),\n",
       " ('Good product. I really liked', 'Positive'),\n",
       " ('I didn’t like product', 'Negative'),\n",
       " ('The product good.', 'Negative')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singh (https://gaganmanku09.medium.com/why-you-should-avoid-removing-stopwords-4fe001d0f5b6):\n",
    "> If you are working with **basic NLP techniques like BOW, Count Vectorizer or TF-IDF(Term Frequency and Inverse Document Frequency)** then removing stopwords is a good idea because stopwords act like noise for these methods. If you working with **LSTMs or other models which capture the semantic meaning and the meaning of a word depends on the context of the previous text**, then it becomes important not to remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "##### 1. For each of the following statements, label them True or False. If False, briefly explain why:\n",
    "\n",
    "\n",
    "A. *If the **F1 score** of a model is **1.0 (100%)**, then the accuracy of your model must also be **100%**.*\n",
    "\n",
    "B. *Stemming **increases the size of the vocabulary** (the vocabulary is the set of all tokens found inside the corpus)*\n",
    "\n",
    "C. *Texts processed using lemmatization will typically have higher recall than stemming.*\n",
    "\n",
    "##### 2. Calculate precison and recall given the following results from a confusion matrix:\n",
    "\n",
    "<img src=\"images/exercise.jpeg\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "##### 3. Provide an example of how stemming can improve recall.\n",
    "\n",
    "##### 4. Provide an example of when stemming might reduce precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization\n",
    "<img src=\"images/count_vectorizer.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use **`sklearn.feature_extraction.text.CountVectorizer`** to easily convert your corpus into a bag of words matrix:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games. Mary does not like football much.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "```\n",
    "Note that the output `X` here is not your traditional Numpy matrix! Calling **`type(X)`** here will yield **`<class 'scipy.sparse.csr.csr_matrix'>`**, which is a **CSR ([compressed sparse row format matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html))**. To convert it into an actual matrix, call the `toarray()` method:\n",
    "\n",
    "```python\n",
    "X.toarray()\n",
    "```\n",
    "Your output will be \n",
    "\n",
    "```\n",
    "array([[0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 1],\n",
    "       [1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1]], dtype=int64)\n",
    "```\n",
    "Notice that using **`X.shape`** $\\rightarrow$ `(2,14)`, indicating a total vocabulary size $V$ of 14. To get what word each of the 14 columns corresponds to, use **`vectorizer.get_feature_names()`**:\n",
    "```\n",
    "['also', 'does', 'football', 'games', 'john', 'like', 'likes', 'mary', 'movies', 'much', 'not', 'to', 'too', 'watch']\n",
    "```\n",
    "\n",
    "Notice, however, that as the vocabulary size $V$ increases, the percent of the matrix taken up by zero values increases:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits.\"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"It's still early, so box-office disappointments are still among the highest-grossing movies of the year.\", \n",
    "        \"That movie was terrific\", \"You love cats\", \n",
    "        \"Pay for top executives at big US companies is vastly higher than what everyday workers make, and a new report from The Wall Street Journal has found that CEOs have hit an eye-popping milestone in the size of their monthly paychecks.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the outputted vectors\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform([\n",
    "    \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "\n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "\n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "\n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>80</th>\n",
       "      <th>90</th>\n",
       "      <th>acquisitions</th>\n",
       "      <th>amortization</th>\n",
       "      <th>analysts</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>because</th>\n",
       "      <th>boils</th>\n",
       "      <th>bps</th>\n",
       "      <th>...</th>\n",
       "      <th>this</th>\n",
       "      <th>three</th>\n",
       "      <th>to</th>\n",
       "      <th>values</th>\n",
       "      <th>we</th>\n",
       "      <th>while</th>\n",
       "      <th>wide</th>\n",
       "      <th>with</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   80  90  acquisitions  amortization  analysts  and  are  because  boils  \\\n",
       "0   0   0             0             0         1    1    0        1      0   \n",
       "1   0   0             0             0         0    0    1        0      0   \n",
       "2   0   0             0             0         0    2    0        0      1   \n",
       "3   1   1             1             1         0    2    0        0      0   \n",
       "\n",
       "   bps  ...  this  three  to  values  we  while  wide  with  year  years  \n",
       "0    0  ...     1      1   1       1   0      0     0     1     1      1  \n",
       "1    0  ...     0      0   1       0   0      0     0     0     0      0  \n",
       "2    0  ...     0      0   3       0   0      0     1     0     0      0  \n",
       "3    2  ...     0      0   2       0   1      1     0     0     0      0  \n",
       "\n",
       "[4 rows x 116 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vectorized corpus into Pandas dataframe\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}