{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Count Vectorization - One Hot Encoding and other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (655, 200)\n",
      "Total number of occurences: 29652\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>agrees</th>\n",
       "      <th>appears</th>\n",
       "      <th>arrive</th>\n",
       "      <th>arrives</th>\n",
       "      <th>asks</th>\n",
       "      <th>attack</th>\n",
       "      <th>attacks</th>\n",
       "      <th>attempt</th>\n",
       "      <th>...</th>\n",
       "      <th>way</th>\n",
       "      <th>wife</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  accidentally  agrees  appears  arrive  arrives  asks  attack  \\\n",
       "0     0             0       0        1       0        0     1       1   \n",
       "1     0             0       0        0       0        0     0       0   \n",
       "2     0             0       1        0       0        0     0       0   \n",
       "3     0             0       0        0       0        1     0       0   \n",
       "4     0             0       0        1       1        0     1       0   \n",
       "\n",
       "   attacks  attempt  ...  way  wife  woman  work  working  world  year  years  \\\n",
       "0        1        0  ...    0     0      1     0        0      0     0      0   \n",
       "1        1        0  ...    1     0      1     0        0      1     0      0   \n",
       "2        0        0  ...    0     1      0     0        0      0     0      0   \n",
       "3        0        0  ...    0     1      0     0        0      0     1      1   \n",
       "4        0        0  ...    1     1      0     0        0      0     0      0   \n",
       "\n",
       "   york  young  \n",
       "0     0      1  \n",
       "1     0      0  \n",
       "2     0      0  \n",
       "3     0      0  \n",
       "4     0      1  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "plots_df = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "# filter only for American movies\n",
    "plots_df = plots_df[plots_df[\"Origin/Ethnicity\"] == \"American\"]\n",
    "\n",
    " # traditional CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    " # use English stopwords, and use one-hot encoding\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=0.05) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "X = vectorizer.fit_transform(plots_df[\"Plot\"])\n",
    "\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {vectorized_df.shape}\")\n",
    "print(f\"Total number of occurences: {vectorized_df.sum().sum()}\")\n",
    "#print(f\"Word counts: {vectorized_df.sum()}\")\n",
    "vectorized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Example\n",
    "\n",
    "### Intro to Algorithmic Marketing:\n",
    "![alt text](images/cos-sim-textbook1.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Magnitude of a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First approach: 3.7416573867739413\n",
      "Second approach: 3.7416573867739413\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def magnitude(x): \n",
    "    return math.sqrt(sum(i**2 for i in x))\n",
    "\n",
    "vectorA = [0,3,1,2]\n",
    "\n",
    "print(f\"First approach: {magnitude(vectorA)}\")\n",
    "print(f\"Second approach: {np.linalg.norm(vectorA)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "It's important to identify a **context window** when analyzing co-occurence. In the image below, the context window size is 4 (2 tokens to either side of the target word):\n",
    "\n",
    "![alt text](images/context_window.png \"Logo Title Text 1\")\n",
    "\n",
    "For the purposes of the next section, we'll define the **entire document as the context window.**\n",
    "\n",
    "Pointwise mutual information measures the ratio between the **joint probability of two events happening** with the probabilities of the two events happening, assuming they are independent. It can be defined with the following equation:\n",
    "\n",
    "$$\n",
    "PMI_{A,B} = log\\frac{p(A,B)}{p(A)p(B)}\n",
    "$$\n",
    "\n",
    "Remember that when two events are independent, $P(i,j) = P(i)P(j)$. Using PMI to just a raw word count is often preferable because very common words have extreme skew (\"the\" and \"of\" will co-occur frequently in the same  )\n",
    "\n",
    "```python\n",
    "import math\n",
    "def pmi(tokenA, tokenB, documents, word_counts):\n",
    "    \n",
    "    # word_counts[token_A] => number of times tokenA appears in the documents\n",
    "    # float(len(documents)) => number of documents\n",
    "    # bigram_freq => a dictionary of the number of times tokenA and tokenB are in the same document together\n",
    "    \n",
    "    prob_A = word_counts[tokenA] / float(len(documents))\n",
    "    prob_B = word_counts[tokenB] / float(len(documents))\n",
    "    prob_A_B = bigram_freq[\" \".join([tokenA, tokenB])] / float(len(documents))\n",
    "    return math.log(prob_A_B/float(prob_A*prob_B),2) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation\n",
    "\n",
    "Many times, in previous homeworks, we've had to manually try to find phrases that belong together. For example, `New York City`.\n",
    "\n",
    "From [nltk.org](http://www.nltk.org/howto/collocations.html), **collocation** can be defined as\n",
    "\n",
    "> expressions of multiple words which commonly co-occur together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "articles = [f\"bbcsport/football/00{i}.txt\" for i in range(1,10)]\n",
    "\n",
    "for article in articles:\n",
    "    article = open(article) # open each sports article\n",
    "    for line in article.readlines():\n",
    "        line = line.replace(\"\\n\", \"\") # replace the new line escape character\n",
    "        if len(line) > 0: # if the line is not empty, process it\n",
    "            line = [lemmatizer.lemmatize(token) for token in word_tokenize(line)] \n",
    "            documents.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "for doc in documents:\n",
    "    new_document = []\n",
    "    for word in doc:\n",
    "        if word.strip().lower() not in stopwords:\n",
    "            new_document.append(word)\n",
    "    new_documents.append(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Champions', 'League'),\n",
       " ('Manchester', 'United'),\n",
       " ('Cristiano', 'Ronaldo'),\n",
       " ('Van', 'Nistelrooy'),\n",
       " ('Wayne', 'Rooney'),\n",
       " ('Alex', 'Ferguson'),\n",
       " ('FA', 'Cup'),\n",
       " ('Ferguson', 'wa'),\n",
       " ('Gary', 'Neville'),\n",
       " ('Man', 'Utd'),\n",
       " ('Manchester', 'City'),\n",
       " ('Sir', 'Alex'),\n",
       " ('national', 'team'),\n",
       " ('wa', \"n't\"),\n",
       " ('23', 'minute')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocation_finder = BigramCollocationFinder.from_documents(new_documents)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency / Inverse Document Frequency\n",
    "\n",
    "\n",
    "## Term Frequency\n",
    "![alt text](images/tf-idf1.png \"Term Frequency\")\n",
    "\n",
    "## Inverse Document Frequency\n",
    "![alt text](images/tf-idf2.png \"Inverse Document Frequency\")\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "![alt text](images/tf-idf4.png \"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn to Generate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,4),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df=0.4, stop_words=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'printr', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")\n",
    "corpus = list(df[\"review\"].values)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1515</th>\n",
       "      <th>1516</th>\n",
       "      <th>1517</th>\n",
       "      <th>1518</th>\n",
       "      <th>1519</th>\n",
       "      <th>1520</th>\n",
       "      <th>1521</th>\n",
       "      <th>1522</th>\n",
       "      <th>1523</th>\n",
       "      <th>1524</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaaaaahhhhhhhhhhh still feel</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaaahhhhhhhhhhh still feel situation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbreviated menu worthy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbreviated menu worthy mcdonald</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc kitchen numerous</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombies bikes stopped stare</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombies little less</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zombies little less predictable</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom line person</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom line person waving</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126454 rows Ã— 1525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0     1     2     3     4     5     \\\n",
       "aaaaaaaahhhhhhhhhhh still feel             0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "aaaaaaaahhhhhhhhhhh still feel situation   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "abbreviated menu worthy                    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "abbreviated menu worthy mcdonald           0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "abc kitchen numerous                       0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...                                        ...   ...   ...   ...   ...   ...   \n",
       "zombies bikes stopped stare                0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zombies little less                        0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zombies little less predictable            0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zoom line person                           0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zoom line person waving                    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                          6     7     8     9     ...  1515  \\\n",
       "aaaaaaaahhhhhhhhhhh still feel             0.0   0.0   0.0   0.0  ...   0.0   \n",
       "aaaaaaaahhhhhhhhhhh still feel situation   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "abbreviated menu worthy                    0.0   0.0   0.0   0.0  ...   0.0   \n",
       "abbreviated menu worthy mcdonald           0.0   0.0   0.0   0.0  ...   0.0   \n",
       "abc kitchen numerous                       0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...                                        ...   ...   ...   ...  ...   ...   \n",
       "zombies bikes stopped stare                0.0   0.0   0.0   0.0  ...   0.0   \n",
       "zombies little less                        0.0   0.0   0.0   0.0  ...   0.0   \n",
       "zombies little less predictable            0.0   0.0   0.0   0.0  ...   0.0   \n",
       "zoom line person                           0.0   0.0   0.0   0.0  ...   0.0   \n",
       "zoom line person waving                    0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "                                          1516  1517  1518  1519  1520  1521  \\\n",
       "aaaaaaaahhhhhhhhhhh still feel             0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "aaaaaaaahhhhhhhhhhh still feel situation   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "abbreviated menu worthy                    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "abbreviated menu worthy mcdonald           0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "abc kitchen numerous                       0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...                                        ...   ...   ...   ...   ...   ...   \n",
       "zombies bikes stopped stare                0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zombies little less                        0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zombies little less predictable            0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zoom line person                           0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zoom line person waving                    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                          1522  1523  1524  \n",
       "aaaaaaaahhhhhhhhhhh still feel             0.0   0.0   0.0  \n",
       "aaaaaaaahhhhhhhhhhh still feel situation   0.0   0.0   0.0  \n",
       "abbreviated menu worthy                    0.0   0.0   0.0  \n",
       "abbreviated menu worthy mcdonald           0.0   0.0   0.0  \n",
       "abc kitchen numerous                       0.0   0.0   0.0  \n",
       "...                                        ...   ...   ...  \n",
       "zombies bikes stopped stare                0.0   0.0   0.0  \n",
       "zombies little less                        0.0   0.0   0.0  \n",
       "zombies little less predictable            0.0   0.0   0.0  \n",
       "zoom line person                           0.0   0.0   0.0  \n",
       "zoom line person waving                    0.0   0.0   0.0  \n",
       "\n",
       "[126454 rows x 1525 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>worst mcdonald ever</th>\n",
       "      <td>2.602793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get order right</th>\n",
       "      <td>2.390093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst mcdonalds ever</th>\n",
       "      <td>2.342385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went drive thru</th>\n",
       "      <td>1.754710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drive thru window</th>\n",
       "      <td>1.624240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need let peeps yelper</th>\n",
       "      <td>0.033990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>part mcwrap could</th>\n",
       "      <td>0.033990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much stinky yes stinky</th>\n",
       "      <td>0.033990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much stinky yes</th>\n",
       "      <td>0.033990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peeking top looking</th>\n",
       "      <td>0.033990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126454 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "worst mcdonald ever     2.602793\n",
       "get order right         2.390093\n",
       "worst mcdonalds ever    2.342385\n",
       "went drive thru         1.754710\n",
       "drive thru window       1.624240\n",
       "...                          ...\n",
       "need let peeps yelper   0.033990\n",
       "part mcwrap could       0.033990\n",
       "much stinky yes stinky  0.033990\n",
       "much stinky yes         0.033990\n",
       "peeking top looking     0.033990\n",
       "\n",
       "[126454 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_csv(\"scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "For the following exercises, use the definitions below:\n",
    "\n",
    "**Term frequency**:\n",
    "$$\n",
    "tf = n(t,d)\n",
    "$$\n",
    "**Inverse document frequency**:\n",
    "$$\n",
    "idf = 1 + \\frac{N}{df(t) + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following exercise, you can perform `lemmatization` and remove `to`, `and`, and `the` as stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"He ate the food\",\n",
    "    \"He liked the meal\",\n",
    "    \"She likes the food\",\n",
    "    \"They like to eat and eat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the TF-IDF score for `eat` in each of the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the corpus below, are `fruit` and `love` independent in terms of their appearance in documents? Prove why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love fruit\",\n",
    "    \"She eats fruit\",\n",
    "    \"They love fruit\",\n",
    "    \"I love the toy\",\n",
    "    \"She ate dinner\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True/False (Explain Why If False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two words `A` and `B` must be independent if they never appear in the same document together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The probability of seeing the word `car` in a document is always equal to or larger than the probability of seing the word `car` given any other word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
