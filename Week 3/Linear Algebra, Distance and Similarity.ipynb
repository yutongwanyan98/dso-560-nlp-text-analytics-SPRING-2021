{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Linear-Algebra\" data-toc-modified-id=\"Linear-Algebra-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Linear Algebra</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dot-Products\" data-toc-modified-id=\"Dot-Products-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dot Products</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-does-a-dot-product-conceptually-mean?\" data-toc-modified-id=\"What-does-a-dot-product-conceptually-mean?-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>What does a dot product conceptually mean?</a></span></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Exercises</a></span></li><li><span><a href=\"#Using-Scikit-Learn\" data-toc-modified-id=\"Using-Scikit-Learn-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Using Scikit-Learn</a></span></li><li><span><a href=\"#Bag-of-Words-Models\" data-toc-modified-id=\"Bag-of-Words-Models-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Bag of Words Models</a></span></li></ul></li><li><span><a href=\"#Distance-Measures\" data-toc-modified-id=\"Distance-Measures-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Distance Measures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Euclidean-Distance\" data-toc-modified-id=\"Euclidean-Distance-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Euclidean Distance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scikit-Learn\" data-toc-modified-id=\"Scikit-Learn-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Scikit Learn</a></span></li></ul></li></ul></li><li><span><a href=\"#Similarity-Measures\" data-toc-modified-id=\"Similarity-Measures-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Similarity Measures</a></span></li><li><span><a href=\"#Linear-Relationships\" data-toc-modified-id=\"Linear-Relationships-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Linear Relationships</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pearson-Correlation-Coefficient\" data-toc-modified-id=\"Pearson-Correlation-Coefficient-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Pearson Correlation Coefficient</a></span><ul class=\"toc-item\"><li><span><a href=\"#Intuition-Behind-Pearson-Correlation-Coefficient\" data-toc-modified-id=\"Intuition-Behind-Pearson-Correlation-Coefficient-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Intuition Behind Pearson Correlation Coefficient</a></span><ul class=\"toc-item\"><li><span><a href=\"#When-$ρ_{Χ_Υ}-=-1$-or--$ρ_{Χ_Υ}-=--1$\" data-toc-modified-id=\"When-$ρ_{Χ_Υ}-=-1$-or--$ρ_{Χ_Υ}-=--1$-4.1.1.1\"><span class=\"toc-item-num\">4.1.1.1&nbsp;&nbsp;</span>When $ρ_{Χ_Υ} = 1$ or  $ρ_{Χ_Υ} = -1$</a></span></li></ul></li></ul></li><li><span><a href=\"#Cosine-Similarity\" data-toc-modified-id=\"Cosine-Similarity-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Cosine Similarity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Shift-Invariance\" data-toc-modified-id=\"Shift-Invariance-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Shift Invariance</a></span></li></ul></li></ul></li><li><span><a href=\"#Exercise:\" data-toc-modified-id=\"Exercise:-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Exercise:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Chi-Square-Test-for-Categorical-Variables\" data-toc-modified-id=\"Chi-Square-Test-for-Categorical-Variables-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Chi Square Test for Categorical Variables</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#3.-Define-your-cosine-similarity-functions\" data-toc-modified-id=\"3.-Define-your-cosine-similarity-functions-5.1.0.1\"><span class=\"toc-item-num\">5.1.0.1&nbsp;&nbsp;</span>3. Define your cosine similarity functions</a></span></li><li><span><a href=\"#4.-Get-the-two-documents-from-the-BoW-feature-space-and-calculate-cosine-similarity\" data-toc-modified-id=\"4.-Get-the-two-documents-from-the-BoW-feature-space-and-calculate-cosine-similarity-5.1.0.2\"><span class=\"toc-item-num\">5.1.0.2&nbsp;&nbsp;</span>4. Get the two documents from the BoW feature space and calculate cosine similarity</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge:-Use-the-Example-Below-to-Create-Your-Own-Cosine-Similarity-Function\" data-toc-modified-id=\"Challenge:-Use-the-Example-Below-to-Create-Your-Own-Cosine-Similarity-Function-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge: Use the Example Below to Create Your Own Cosine Similarity Function</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Create-a-list-of-all-the-vocabulary-$V$\" data-toc-modified-id=\"Create-a-list-of-all-the-vocabulary-$V$-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Create a list of all the <strong>vocabulary $V$</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Native-Implementation:\" data-toc-modified-id=\"Native-Implementation:-6.0.1.1\"><span class=\"toc-item-num\">6.0.1.1&nbsp;&nbsp;</span>Native Implementation:</a></span></li></ul></li><li><span><a href=\"#Create-your-Bag-of-Words-model\" data-toc-modified-id=\"Create-your-Bag-of-Words-model-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Create your Bag of Words model</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "In the natural language processing, each document is a vector of numbers.\n",
    "\n",
    "\n",
    "## Dot Products\n",
    "\n",
    "A dot product is defined as\n",
    "\n",
    "$ a \\cdot b = \\sum_{i}^{n} a_{i}b_{i} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + \\dots + a_{n}b_{n}$\n",
    "\n",
    "The geometric definition of a dot product is \n",
    "\n",
    "$ a \\cdot b = $\\|\\|b\\|\\|\\|\\|a\\|\\|\n",
    "\n",
    "### What does a dot product conceptually mean?\n",
    "\n",
    "A dot product is a representation of the **similarity between two components**, because it is calculated based upon shared elements. It tells you how much one vector goes in the direction of another vector.\n",
    "\n",
    "The actual value of a dot product reflects the direction of change:\n",
    "\n",
    "* **Zero**: we don't have any growth in the original direction\n",
    "* **Positive** number: we have some growth in the original direction\n",
    "* **Negative** number: we have negative (reverse) growth in the original direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0,2]\n",
    "B = [0,1]\n",
    "\n",
    "def dot_product(x,y):\n",
    "    return sum(a*b for a,b in zip(x,y))\n",
    "\n",
    "dot_product(A,B)\n",
    "# What will the dot product of A and B be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Correlations](images/dot_product.png \"Visualization of various r values for Pearson correlation coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will the dot product of `A` and `B` be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1,2]\n",
    "B = [2,4]\n",
    "dot_product(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will the dot product of `document_1` and `document_2` be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_1 = [0, 0, 1]\n",
    "document_2 = [1, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\",\n",
    "              \"John also likes football. Mary doesn't like football.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(data_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits. \"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "\n",
    "zeroes = np.where(X.flatten() == 0)[0].size \n",
    "percent_sparse = zeroes / X.size\n",
    "print(f\"The bag of words feature space is {round(percent_sparse * 100,2)}% sparse. \\n\\\n",
    "That's approximately {round(getsizeof(X) * percent_sparse,2)} bytes of wasted memory. This is why sklearn uses CSR (compressed sparse rows) instead of normal matrices!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measures\n",
    "\n",
    "\n",
    "## Euclidean Distance\n",
    "\n",
    "Euclidean distances can range from 0 (completely identically) to $\\infty$ (extremely dissimilar). \n",
    "\n",
    "The distance between two points, $x$ and $y$, can be defined as $d(x,y)$:\n",
    "\n",
    "$$\n",
    "d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_{i}-y_{i})^2}\n",
    "$$\n",
    "\n",
    "Compared to the other dominant distance measure (cosine similarity), **magnitude** plays an extremely important role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    " \n",
    "def euclidean_distance_1(x,y):\n",
    "    distance = sum((a-b)**2 for a, b in zip(x, y))\n",
    "    return sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's typically an easier way to write this function that takes advantage of Numpy's vectorization capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def euclidean_distance_2(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return np.linalg.norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = [document_1, document_2]\n",
    "euclidean_distances(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures\n",
    "\n",
    "Similarity measures will always range between -1 and 1. A similarity of -1 means the two objects are complete opposites, while a similarity of 1 indicates the objects are identical.\n",
    "\n",
    "\n",
    "# Linear Relationships\n",
    "\n",
    "## Pearson Correlation Coefficient\n",
    "* We use **ρ** when the correlation is being measured from the population, and **r** when it is being generated from a sample.\n",
    "* An r value of 1 represents a **perfect linear** relationship, and a value of -1 represents a perfect inverse linear relationship.\n",
    "\n",
    "The equation for Pearson's correlation coefficient is \n",
    "$$\n",
    "ρ_{Χ_Υ} = \\frac{cov(X,Y)}{σ_Xσ_Y}\n",
    "$$\n",
    "\n",
    "### Intuition Behind Pearson Correlation Coefficient\n",
    "\n",
    "#### When $ρ_{Χ_Υ} = 1$ or  $ρ_{Χ_Υ} = -1$\n",
    "\n",
    "This requires **$cov(X,Y) = σ_Xσ_Y$** or **$-1 * cov(X,Y) = σ_Xσ_Y$** (in the case of $ρ = -1$) . This corresponds with all the data points lying perfectly on the same line.\n",
    "![Correlations](images/correlation.png \"Visualization of various r values for Pearson correlation coefficient\")\n",
    "\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "The cosine similarity of two vectors (each vector will usually represent one document) is a measure that calculates $ cos(\\theta)$, where $\\theta$ is the angle between the two vectors.\n",
    "\n",
    "Therefore, if the vectors are **orthogonal** to each other (90 degrees), $cos(90) = 0$. If the vectors are in exactly the same direction, $\\theta = 0$ and $cos(0) = 1$.\n",
    "\n",
    "Cosine similiarity **does not care about the magnitude of the vector, only the direction** in which it points. This can help normalize when comparing across documents that are different in terms of word count.\n",
    "\n",
    "![Cosine Similarity](images/cos-equation.png)\n",
    "\n",
    "### Shift Invariance\n",
    "\n",
    "* The Pearson correlation coefficient between X and Y does not change with you transform $X \\rightarrow a + bX$ and $Y \\rightarrow c + dY$, assuming $a$, $b$, $c$, and $d$ are constants and $b$ and $d$ are positive.\n",
    "* Cosine similarity does, however, change when transformed in this way.\n",
    "\n",
    "\n",
    "<h1>Exercise:</h1>\n",
    "\n",
    ">In Python, find the **cosine similarity** and the **Pearson correlation coefficient** of the two following sentences, assuming a **one-hot encoded binary bag of words** model. You may use a library to create the BoW feature space, but do not use libraries other than `numpy` or `scipy` to compute Pearson and cosine similarity:\n",
    "\n",
    ">`A = \"John likes to watch movies. Mary likes movies too\"`\n",
    "\n",
    ">`B = \"John also likes to watch football games, but he likes to watch movies on occasion as well\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi Square Test for Categorical Variables\n",
    "Generally speaking, we do not want to use Pearson correlation for comparing the linear relationship between two categorical variables (with 0s and 1s). We can use a Chi Square test to approximate the relationship between two categorical variables, in this case between the target `0` or `1`, and the text token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample dataset with only 4 records\n",
    "dataset = [(\"I hate this movie\", 0), (\"She hate it\",0), \n",
    "           (\"I love it\",1), (\"love this movie\",1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "features = vectorizer.fit_transform(list(map(lambda row: row[0], dataset))).toarray()\n",
    "data = pd.DataFrame(features, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"_TARGET\"] = pd.Series(list(map(lambda row: row[1], dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[\"_TARGET\"])\n",
    "y = data[\"_TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# k = 2 tells four top features to be selected\n",
    "# Score function Chi2 tells the feature to be selected using Chi Square\n",
    "test = SelectKBest(score_func=chi2, k=2)\n",
    "fit = test.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define your cosine similarity functions\n",
    "\n",
    "```python\n",
    "from scipy.spatial.distance import cosine # we are importing this library to check that our own cosine similarity func works\n",
    "from numpy import dot # to calculate dot product\n",
    "from numpy.linalg import norm # to calculate the norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    numerator = dot(A, B)\n",
    "    denominator = norm(A) * norm(B)\n",
    "    return numerator / denominator\n",
    "\n",
    "def cosine_distance(A,B):\n",
    "    return 1 - cosine_similarity\n",
    "\n",
    "A = [0,2,3,4,1,2]\n",
    "B = [1,3,4,0,0,2]\n",
    "\n",
    "# check that your native implementation and 3rd party library function produce the same values\n",
    "assert round(cosine_similarity(A,B),4) ==  round(cosine(A,B),4)\n",
    "```\n",
    "\n",
    "#### 4. Get the two documents from the BoW feature space and calculate cosine similarity\n",
    "\n",
    "```python\n",
    "cosine_similarity(X[0], X[1])\n",
    "```\n",
    ">0.5241424183609592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from numpy import dot\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    numerator = dot(A, B)\n",
    "    denominator = norm(A) * norm(B)\n",
    "    return numerator / denominator # remember, you take 1 - the distance to get the distance\n",
    "\n",
    "def cosine_distance(A,B):\n",
    "    return 1 - cosine_similarity\n",
    "\n",
    "A = [0,2,3,4,1,2]\n",
    "B = [1,3,4,0,0,2]\n",
    "\n",
    "# check that your native implementation and 3rd party library function produce the same values\n",
    "assert round(cosine_similarity(A,B),4) ==  round(1 - cosine(A,B),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# take two very similar sentences, should have high similarity\n",
    "# edit these sentences to become less similar, and the similarity score should decrease\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games\"]\n",
    "\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "X = X.toarray()\n",
    "print(vectorizer.get_feature_names())\n",
    "cosine_similarity(X[0], X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Use the Example Below to Create Your Own Cosine Similarity Function\n",
    "\n",
    "### Create a list of all the **vocabulary $V$**\n",
    "\n",
    "Using **`sklearn`**'s **`CountVectorizer`**:\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too\", \n",
    "\"John also likes to watch football games, but he likes to watch movies on occasion as well\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "V = vectorizer.get_feature_names()\n",
    "```\n",
    "\n",
    "#### Native Implementation:\n",
    "```python\n",
    "def get_vocabulary(sentences):\n",
    "    vocabulary = {} # create an empty set - question: Why not a list?\n",
    "    for sentence in sentences:\n",
    "         # this is a very crude form of \"tokenization\", would not actually use in production\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.add(word)\n",
    "    return vocabulary\n",
    "```\n",
    "\n",
    "### Create your Bag of Words model\n",
    "```python\n",
    "X = X.toarray()\n",
    "print(X)\n",
    "```\n",
    "Your console output:\n",
    "```python\n",
    "[[0 0 0 1 2 1 2 1 1 1]\n",
    " [1 1 1 1 1 0 0 1 0 1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [[0,0,0,1,2,1,2,1,1,1],\n",
    "           [1,1,1,1,1,0,0,1,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find_norm(vector):\n",
    "    total = 0\n",
    "    for element in vector:\n",
    "        total += element ** 2\n",
    "        \n",
    "    return math.sqrt(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "norm(vectors[0]) # Numpy\n",
    "find_norm(vectors[0]) # your own\n",
    "\n",
    "assert norm(vectors[0]) == find_norm(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product(vectors[0], vectors[1]) / (find_norm(vectors[0]) * find_norm(vectors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "cosine_similarity(vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
