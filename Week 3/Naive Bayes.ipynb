{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (\"I ate dinner early\", \"HAM\"),\n",
    "    (\"free money today\", \"SPAM\"),\n",
    "    (\"I had a blast\", \"HAM\"),\n",
    "    (\"sign up free early today\", \"HAM\"),\n",
    "    (\"only free today\", \"SPAM\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = set()\n",
    "\n",
    "# Build corpus\n",
    "for document in documents:\n",
    "    text = document[0]\n",
    "    class_value = document[1]\n",
    "    for word in text.split():\n",
    "        corpus.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Conditional Probabilities\n",
    "We need to generate first $P(x|y)$. For instance, what is the likelihood of finding the word `free` if we know the document is `HAM` is represented as `P(x=\"free\"|y=\"HAM\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                         columns=[\"likelihood_given_ham\", \"likelihood_given_spam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_documents = 0\n",
    "ham_documents = 0\n",
    "for document in documents:\n",
    "    if document[1] == \"SPAM\":\n",
    "        spam_documents += 1\n",
    "    else:\n",
    "        ham_documents += 1\n",
    "\n",
    "    print(f\"{document}\")\n",
    "    print(f\"Spam documents: {spam_documents}\")\n",
    "    print(f\"Ham documents: {ham_documents} \\n\\n\")\n",
    "    \n",
    "p_ham = ham_documents / (spam_documents + ham_documents)\n",
    "p_spam = spam_documents / (spam_documents + ham_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in corpus:\n",
    "    \n",
    "    ham_documents_with_word = 0\n",
    "    spam_documents_with_word = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        document_class = document[1]\n",
    "        if word in document[0].split():\n",
    "            if document[1] == \"HAM\":\n",
    "                ham_documents_with_word += 1\n",
    "            else:\n",
    "                spam_documents_with_word += 1\n",
    "    \n",
    "    print(f\"For word {word}, {ham_documents_with_word} ham out of {ham_documents} ham documents.\")\n",
    "    print(f\"For word {word}, {spam_documents_with_word} spam out of {spam_documents} spam documents.\\n\")\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_ham\"] = ham_documents_with_word * 1.0 / ham_documents\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_spam\"] = spam_documents_with_word * 1.0 / spam_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = \"free today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(test_document, conditional_probabilities):\n",
    "    likelihood_ham = 1\n",
    "    likelihood_spam = 1\n",
    "    for word in test_document.split():\n",
    "        likelihood_ham = likelihood_ham * conditional_probabilities.loc[word, \"likelihood_given_ham\"]\n",
    "        likelihood_spam = likelihood_spam * conditional_probabilities.loc[word, \"likelihood_given_spam\"]\n",
    "    \n",
    "    return likelihood_ham, likelihood_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_ham, likelihood_spam = get_likelihood(test_document, conditional_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(likelihood_ham, likelihood_spam, p_ham, p_spam):\n",
    "    posterior_ham = likelihood_ham * p_ham / (likelihood_ham * p_ham + likelihood_spam * p_spam)\n",
    "    posterior_spam = likelihood_spam * p_spam / (likelihood_ham * p_ham + likelihood_spam * p_spam)\n",
    "    return posterior_ham, posterior_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_posterior(likelihood_ham, likelihood_spam, p_ham, p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(documents):\n",
    "    corpus = set()\n",
    "    # Build corpus\n",
    "    for document in documents:\n",
    "        text = document[0]\n",
    "        class_value = document[1]\n",
    "        for word in text.split():\n",
    "            corpus.add(word)\n",
    "    \n",
    "    conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                             columns=[\"likelihood_given_ham\", \"likelihood_given_spam\"])\n",
    "    \n",
    "    spam_documents = 0\n",
    "    ham_documents = 0\n",
    "    for document in documents:\n",
    "        if document[1] == \"SPAM\":\n",
    "            spam_documents += 1\n",
    "        else:\n",
    "            ham_documents += 1\n",
    "    p_ham = ham_documents / (spam_documents + ham_documents)\n",
    "    p_spam = spam_documents / (spam_documents + ham_documents)\n",
    "    \n",
    "    for word in corpus:\n",
    "        ham_documents_with_word = 0\n",
    "        spam_documents_with_word = 0\n",
    "    \n",
    "        for document in documents:\n",
    "            document_class = document[1]\n",
    "            if word in document[0].split():\n",
    "                if document[1] == \"HAM\":\n",
    "                    ham_documents_with_word += 1\n",
    "                else:\n",
    "                    spam_documents_with_word += 1\n",
    "\n",
    "        #print(f\"For word {word}, {ham_documents_with_word} ham out of {ham_documents}.\")\n",
    "        #print(f\"For word {word}, {spam_documents_with_word} spam out of {spam_documents}.\")\n",
    "        conditional_probabilities.loc[word, \"likelihood_given_ham\"] = ham_documents_with_word * 1.0 / ham_documents\n",
    "        conditional_probabilities.loc[word, \"likelihood_given_spam\"] = spam_documents_with_word * 1.0 / spam_documents\n",
    "\n",
    "    \n",
    "    return conditional_probabilities, p_ham, p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_naive_bayes(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Non-Existent Words\n",
    "\n",
    "From [Sebastian Raschka, Python Machine Learning](https://arxiv.org/pdf/1410.5329.pdf)\n",
    "![Correlations](images/smoothing.png \"Visualization of various r values for Pearson correlation coefficient\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
