{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Word-Count\" data-toc-modified-id=\"Word-Count-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Word Count</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-Counter\" data-toc-modified-id=\"Using-Counter-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Using <code>Counter</code></a></span></li><li><span><a href=\"#Adding-Word-Counts-From-Two-Distinct-Datasets-Together\" data-toc-modified-id=\"Adding-Word-Counts-From-Two-Distinct-Datasets-Together-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Adding Word Counts From Two Distinct Datasets Together</a></span></li></ul></li><li><span><a href=\"#Removing-Stopwords-Using-gensim\" data-toc-modified-id=\"Removing-Stopwords-Using-gensim-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Removing Stopwords Using <code>gensim</code></a></span></li><li><span><a href=\"#Finding-Similar-Word-Matches-Using-difflib\" data-toc-modified-id=\"Finding-Similar-Word-Matches-Using-difflib-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Finding Similar Word Matches Using <code>difflib</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Fuzzy-Matching\" data-toc-modified-id=\"Fuzzy-Matching-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Fuzzy Matching</a></span><ul class=\"toc-item\"><li><span><a href=\"#Use-Cases\" data-toc-modified-id=\"Use-Cases-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Use Cases</a></span></li><li><span><a href=\"#Limitations\" data-toc-modified-id=\"Limitations-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Limitations</a></span></li><li><span><a href=\"#Slow-Performance\" data-toc-modified-id=\"Slow-Performance-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Slow Performance</a></span></li><li><span><a href=\"#Not-&quot;Language-Aware&quot;\" data-toc-modified-id=\"Not-&quot;Language-Aware&quot;-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Not \"Language Aware\"</a></span></li></ul></li><li><span><a href=\"#Install-the-Dependencies-if-Necessary\" data-toc-modified-id=\"Install-the-Dependencies-if-Necessary-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Install the Dependencies if Necessary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Token-Set-Ratio-(following-examples-from-fuzzywuzzy's-documentation)\" data-toc-modified-id=\"Token-Set-Ratio-(following-examples-from-fuzzywuzzy's-documentation)-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Token Set Ratio (following examples from <code>fuzzywuzzy</code>'s documentation)</a></span></li><li><span><a href=\"#Token-Set-Ratio\" data-toc-modified-id=\"Token-Set-Ratio-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Token Set Ratio</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "## Using `Counter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal dictionary object will return a key error if you do not first initialize the key value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'yu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5b4cffd8d4f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mordinary_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mordinary_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'yu'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "ordinary_dict: Dict = dict()\n",
    "ordinary_dict[\"yu\"] = 0\n",
    "ordinary_dict[\"yu\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Counter` object in `collections` has a default value of 0 for every key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'yu': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "counter[\"yu\"] += 1\n",
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the you can pass in a list of strings to the `Counter` constructor, as well as calling the\n",
    "`most_common` method to get the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7363), ('and', 4727), ('of', 3944), ('to', 3398), ('a', 2792)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "words: List[str] = open(\"tale-of-two-cities.txt\").read().split()\n",
    "dickens_counter = Counter(words)\n",
    "dickens_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also quickly use this counter to find the percentage of words in a corpus that belong to a certain word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054036400998091885"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dickens_counter[\"the\"] / sum(dickens_counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Word Counts From Two Distinct Datasets Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add two `Counter` objects together to get their combined counts. In this example, we'll load in the `fraudulent_emails.txt` dataset and start a new counter called `email_counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 141), ('to', 116), ('I', 115), ('of', 80), ('in', 80)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_counter = Counter(open(\"fraudulent_emails.txt\").read().split())\n",
    "email_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7504), ('and', 4789), ('of', 4024), ('to', 3514), ('a', 2834)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_counter: Counter = dickens_counter + email_counter\n",
    "combined_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also subtract counts from one dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 141), ('to', 116), ('I', 115), ('of', 80), ('in', 80)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get back the original email_counter\n",
    "(combined_counter - dickens_counter).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords Using `gensim`\n",
    "\n",
    "Removing stopwords in `nltk` often means you first have to tokenize the document into distinct tokens, then run each token through to check if it is a stopword. Another commonly used NLP library in Python, `gensim`, has a helper function to do this all in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rendered manner desperate, state beckoning conductor, drew neck arm shook shoulder, lifted little, hurried room. He sat door, held her, clinging him.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "text = '''\n",
    "Rendered in a manner desperate, by her state and by the beckoning of their conductor,\n",
    "he drew over his neck the arm that shook upon his shoulder, lifted her a little, and hurried \n",
    "her into the room. He sat her down just within the door, and held her, clinging to him.\n",
    "'''\n",
    "processed_text = remove_stopwords(text)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, this only works well if you are happy with Gensim's only predefined list of stopwords. To inspect what stopwords are used in Gensim, use\n",
    "```python\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "print(STOPWORDS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Word Matches Using `difflib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within Python's Standard Library, the `difflib` has a variety of tools for helping identify differences between text and content. It uses an algorithm called the **Ratcliff-Obershelp algorithm**, which is described in brief below:\n",
    "\n",
    "> The idea is to find the longest contiguous matching subsequence that contains no “junk” elements; these “junk” elements are ones that are uninteresting in some sense, such as blank lines or whitespace. (Handling junk is an extension to the Ratcliff and Obershelp algorithm.) The same idea is then applied recursively to the pieces of the sequences to the left and to the right of the matching subsequence. This does not yield minimal edit sequences, but does tend to yield matches that “look right” to people. [Link](https://docs.python.org/3/library/difflib.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '20k.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-72c4af50fc19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this loads in the top 20k most popular words in the English language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"20k.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20k.txt'"
     ]
    }
   ],
   "source": [
    "# this loads in the top 20k most popular words in the English language\n",
    "words = set(map(lambda word: word.replace(\"\\n\", \"\"), open(\"20k.txt\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight', 'night']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "w = \"knaght\"\n",
    "difflib.get_close_matches(w, [\"knight\", \"cat\", \"night\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine this with a tokenizer to create your own (very basic) spellcheck function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is a crazy person'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def spellcheck_document(text):\n",
    "    new_tokens = []\n",
    "    for token in word_tokenize(text):\n",
    "        matches = difflib.get_close_matches(token.lower(), words, n=1, cutoff=0.7)\n",
    "        if len(matches) == 0 or token.lower() in words:\n",
    "            new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(matches[0])\n",
    "    return \" \".join(new_tokens)\n",
    "spellcheck_document(\"He is a craezy perzon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy matching refers to \"approximate matching\", where we are allowed a certain degree of error between the query value and the search result. \n",
    "\n",
    "The `fuzzywuzzy` library uses a distance measure called **Levenshtein Distance** which describes the minimum number of operations to transform one string into another.\n",
    "\n",
    "* `cat` $\\rightarrow$ `cat` : `0` distance\n",
    "* `dog` $\\rightarrow$ `door`: `2` distance\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "* spell checking\n",
    "* DNA analysis\n",
    "* authorship/plagiarism detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads in the top 20k most popular words in the English language\n",
    "words = set(map(lambda word: word.replace(\"\\n\", \"\"), open(\"20k.txt\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b49ec418e971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractBests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Time in seconds to check 10 words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "target = \"kerfuffled\"\n",
    "\n",
    "\n",
    "start = timer()\n",
    "for i in range(10):\n",
    "    bests = process.extractBests(target, words, scorer=fuzz.ratio)\n",
    "end = timer()\n",
    "print(end - start) # Time in seconds to check 10 words\n",
    "print(f'Best results: {bests}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"kerfuffled\", \"ruled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not \"Language Aware\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparing the classification proposed by the Levenshtein distance to that of the comparative method shows that the Levenshtein classification is correct only 40% of time. Standardizing the orthography increases the performance, but only to a maximum of 65% accuracy within language subgroups. The accuracy of the Levenshtein classification **decreases rapidly with phylogenetic distance**, failing to discriminate homology and chance similarity across distantly related languages.This poor performance suggests the need for more linguistically nuanced methods for automated language classification tasks.\n",
    "\n",
    "[\"Levenshtein distances fail to identify language relationships accurately\" by Simon Greenhill](https://dl.acm.org/doi/10.1162/COLI_a_00073)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Dependencies if Necessary\n",
    "```python\n",
    "!pip3 install fuzzywuzzy\n",
    "!pip3 install python-Levenshtein\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"cat\", \"saturday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"dog\", \"hog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"smithy\", \"smithfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is it symmetric?\n",
    "fuzz.ratio(\"smithfield\", \"smithy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"photosynthesis\", \"photosynthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does case matter?\n",
    "fuzz.ratio(\"Photosynthesis\", \"photosynthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what happens if you arbitrarily increase the size of the strings?\n",
    "\n",
    "fuzz.ratio(\"dog\" * 3, \"hog\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Set Ratio (following examples from `fuzzywuzzy`'s documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\"))\n",
    "print(fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Set Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\"))\n",
    "print(fuzz.token_sort_ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "126.52px",
    "left": "1460.98px",
    "top": "197.952px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
